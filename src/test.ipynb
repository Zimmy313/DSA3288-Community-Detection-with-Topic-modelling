{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from hlda import nCRPTree, Node\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fetch the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "raw_corpus = newsgroups.data\n",
    "\n",
    "# Step 2: Define a preprocessing pipeline\n",
    "def preprocess_corpus(documents, stop_words):\n",
    "    \"\"\"\n",
    "    Preprocess the corpus by:\n",
    "    - Lowercasing\n",
    "    - Tokenizing\n",
    "    - Removing non-alpha tokens\n",
    "    - Removing stopwords\n",
    "    \"\"\"\n",
    "    preprocessed = []\n",
    "    for doc in documents:\n",
    "        tokens = word_tokenize(doc.lower())\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "        preprocessed.append(tokens)\n",
    "    return preprocessed\n",
    "\n",
    "# Create a set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the corpus\n",
    "preprocessed_corpus = preprocess_corpus(raw_corpus, stop_words)\n",
    "\n",
    "# We have a very large corpus here. Let's use a small subset for demonstration:\n",
    "subset_size = 50\n",
    "subset_corpus = preprocessed_corpus[:subset_size]\n",
    "\n",
    "# Build a vocabulary from the subset\n",
    "vocab = sorted(set(word for doc in subset_corpus for word in doc))\n",
    "\n",
    "# Step 3: Initialize and run hLDA with a limited number of iterations\n",
    "tree = nCRPTree(\n",
    "    gamma=1.0,\n",
    "    eta=0.1,\n",
    "    num_levels=10,   # You can adjust the number of levels\n",
    "    vocab=vocab,\n",
    "    m=0.5,\n",
    "    pi=1.0\n",
    ")\n",
    "\n",
    "# Run a small number of Gibbs iterations for testing\n",
    "num_iterations = 100\n",
    "burn_in = 10\n",
    "thinning = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 completed.\n",
      "Burn-in period of 10 iterations completed.\n",
      "Iteration 20 completed.\n",
      "Iteration 30 completed.\n",
      "Iteration 40 completed.\n",
      "Iteration 50 completed.\n",
      "Iteration 60 completed.\n",
      "Iteration 70 completed.\n",
      "Iteration 80 completed.\n",
      "Iteration 90 completed.\n",
      "Iteration 100 completed.\n",
      "Gibbs sampling completed.\n"
     ]
    }
   ],
   "source": [
    "tree.gibbs_sampling(subset_corpus, num_iterations=num_iterations, burn_in=burn_in, thinning=thinning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling completed. Now generating the tree visualization...\n",
      "Tree visualization saved as ncrp_tree_example.png\n",
      "Visualization generated: ncrp_tree_example.png\n"
     ]
    }
   ],
   "source": [
    "print(\"Sampling completed. Now generating the tree visualization...\")\n",
    "\n",
    "# Step 4: Visualize the resulting tree\n",
    "print_tree_graphviz(tree.root, vocab, filename='ncrp_tree_example', view=False)\n",
    "print(\"Visualization generated: ncrp_tree_example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](../image/ncrp_tree_example.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA3288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
