{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from hlda import nCRPTree\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fetch the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "raw_corpus = newsgroups.data\n",
    "\n",
    "# Step 2: Define a preprocessing pipeline\n",
    "def preprocess_corpus(documents, stop_words):\n",
    "    \"\"\"\n",
    "    Preprocess the corpus by:\n",
    "    - Lowercasing\n",
    "    - Tokenizing\n",
    "    - Removing non-alpha tokens\n",
    "    - Removing stopwords\n",
    "    \"\"\"\n",
    "    preprocessed = []\n",
    "    for doc in documents:\n",
    "        tokens = word_tokenize(doc.lower())\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "        preprocessed.append(tokens)\n",
    "    return preprocessed\n",
    "\n",
    "# Create a set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the corpus\n",
    "preprocessed_corpus = preprocess_corpus(raw_corpus, stop_words)\n",
    "\n",
    "# Using only a subset of the corpus\n",
    "subset_size = 100\n",
    "subset_corpus = preprocessed_corpus[:subset_size]\n",
    "\n",
    "# Build a vocabulary from the subset\n",
    "vocab = sorted(set(word for doc in subset_corpus for word in doc))\n",
    "\n",
    "# Step 3: Initialize and run hLDA with a limited number of iterations\n",
    "tree = nCRPTree(\n",
    "    gamma=1.0,\n",
    "    eta=0.1,\n",
    "    num_levels=20,   # Max_level\n",
    "    vocab=vocab,\n",
    "    m=0.5,\n",
    "    pi=1.0\n",
    ")\n",
    "\n",
    "# Run a small number of Gibbs iterations for testing\n",
    "num_iterations = 1000\n",
    "burn_in = 100\n",
    "thinning = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gibbs sampling completed.\n"
     ]
    }
   ],
   "source": [
    "tree.gibbs_sampling(subset_corpus, num_iterations=num_iterations, burn_in=burn_in, thinning=thinning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling completed.\n",
      "Tree visualization saved as ncrp_tree_example.png\n"
     ]
    }
   ],
   "source": [
    "print(\"Sampling completed.\")\n",
    "\n",
    "# Step 4: Visualize the resulting tree\n",
    "print_tree_graphviz(tree.root, vocab, filename='ncrp_tree_example', view=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](../image/ncrp_tree_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# A hierarchical topic tree with associated words\n",
    "topic_tree = {\n",
    "    \"_words\": {\n",
    "        \"abstract\": 10, \"concept\": 8, \"entity\": 5, \"system\": 7, \"structure\": 6,\n",
    "        \"analysis\": 5, \"data\": 5, \"model\": 5, \"process\": 5, \"method\": 4\n",
    "    },\n",
    "    \"Science\": {\n",
    "        \"_words\": {\n",
    "            \"science\": 10, \"theory\": 8, \"experiment\": 7, \"research\": 7, \"hypothesis\": 6,\n",
    "            \"laboratory\": 5, \"data\": 5, \"statistic\": 4, \"evidence\": 4\n",
    "        },\n",
    "        \"Physics\": {\n",
    "            \"_words\": {\n",
    "                \"force\": 10, \"energy\": 8, \"quantum\": 7, \"particle\": 7, \"field\": 6, \n",
    "                \"relativity\": 5, \"momentum\": 5, \"photon\": 5, \"wave\": 5, \"magnetism\": 4\n",
    "            },\n",
    "            \"Astrophysics\": {\n",
    "                \"_words\": {\n",
    "                    \"galaxy\": 10, \"cosmic\": 8, \"nebula\": 7, \"cosmology\": 7, \"darkmatter\": 6, \n",
    "                    \"supernova\": 5, \"telescope\": 5, \"exoplanet\": 5, \"gravity\": 5, \"orbit\": 4\n",
    "                }\n",
    "            },\n",
    "            \"Particle Physics\": {\n",
    "                \"_words\": {\n",
    "                    \"hadron\": 10, \"boson\": 8, \"fermion\": 7, \"quark\": 7, \"collider\": 6,\n",
    "                    \"neutrino\": 5, \"charm\": 5, \"lepton\": 5, \"spin\": 5, \"muon\": 4\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Chemistry\": {\n",
    "            \"_words\": {\n",
    "                \"molecule\": 10, \"reaction\": 8, \"compound\": 7, \"acid\": 7, \"base\": 6, \n",
    "                \"electron\": 5, \"proton\": 5, \"catalyst\": 5, \"radical\": 5, \"solvent\": 4\n",
    "            },\n",
    "            \"Organic Chemistry\": {\n",
    "                \"_words\": {\n",
    "                    \"carbon\": 10, \"hydrocarbon\": 8, \"polymer\": 7, \"enzyme\": 7, \"synthesis\": 6,\n",
    "                    \"aminoacid\": 5, \"peptide\": 5, \"aldehyde\": 5, \"ketone\": 5, \"ester\": 4\n",
    "                }\n",
    "            },\n",
    "            \"Inorganic Chemistry\": {\n",
    "                \"_words\": {\n",
    "                    \"metal\": 10, \"alloy\": 8, \"mineral\": 7, \"oxide\": 7, \"salt\": 6,\n",
    "                    \"ion\": 5, \"complex\": 5, \"crystal\": 5, \"cluster\": 5, \"silicate\": 4\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Biology\": {\n",
    "            \"_words\": {\n",
    "                \"cell\": 10, \"gene\": 8, \"evolution\": 7, \"species\": 7, \"ecosystem\": 6,\n",
    "                \"organism\": 5, \"metabolism\": 5, \"phylum\": 5, \"genome\": 5, \"bacteria\": 4\n",
    "            },\n",
    "            \"Microbiology\": {\n",
    "                \"_words\": {\n",
    "                    \"virus\": 10, \"fungus\": 8, \"plasmid\": 7, \"antibiotic\": 7, \"microbe\": 6,\n",
    "                    \"biofilm\": 5, \"pathogen\": 5, \"yeast\": 5, \"protist\": 5, \"spore\": 4\n",
    "                }\n",
    "            },\n",
    "            \"Neuroscience\": {\n",
    "                \"_words\": {\n",
    "                    \"neuron\": 10, \"synapse\": 8, \"cortex\": 7, \"axon\": 7, \"neurotransmitter\": 6,\n",
    "                    \"gliacell\": 5, \"cognition\": 5, \"memory\": 5, \"perception\": 5, \"nervous\": 4\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"Sports\": {\n",
    "        \"_words\": {\n",
    "            \"competition\": 10, \"team\": 9, \"player\": 9, \"coach\": 7, \"tournament\": 6,\n",
    "            \"league\": 5, \"training\": 5, \"score\": 5, \"referee\": 5, \"stadium\": 4\n",
    "        },\n",
    "        \"Football\": {\n",
    "            \"_words\": {\n",
    "                \"ball\": 10, \"goal\": 8, \"pitch\": 7, \"tackle\": 7, \"striker\": 6,\n",
    "                \"midfielder\": 5, \"defender\": 5, \"penalty\": 5, \"corner\": 5, \"foul\": 4\n",
    "            },\n",
    "            \"American Football\": {\n",
    "                \"_words\": {\n",
    "                    \"quarterback\": 10, \"touchdown\": 8, \"linebacker\": 7, \"helmet\": 7, \"endzone\": 6,\n",
    "                    \"scrimmage\": 5, \"fumble\": 5, \"receiver\": 5, \"fieldgoal\": 5, \"blitz\": 4\n",
    "                }\n",
    "            },\n",
    "            \"Soccer\": {\n",
    "                \"_words\": {\n",
    "                    \"football\": 10, \"soccer\": 8, \"worldcup\": 7, \"uefa\": 7, \"maradona\": 6,\n",
    "                    \"ronaldo\": 5, \"messi\": 5, \"beckham\": 5, \"fifa\": 5, \"leaguecup\": 4\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Basketball\": {\n",
    "            \"_words\": {\n",
    "                \"basket\": 10, \"dribble\": 8, \"hoop\": 7, \"court\": 7, \"dunk\": 6,\n",
    "                \"rebound\": 5, \"assist\": 5, \"guard\": 5, \"forward\": 5, \"center\": 4\n",
    "            },\n",
    "            \"NBA\": {\n",
    "                \"_words\": {\n",
    "                    \"nba\": 10, \"lakers\": 8, \"bulls\": 7, \"celtics\": 7, \"knicks\": 6,\n",
    "                    \"lebron\": 5, \"jordan\": 5, \"kobe\": 5, \"shaquille\": 5, \"warriors\": 4\n",
    "                }\n",
    "            },\n",
    "            \"FIBA\": {\n",
    "                \"_words\": {\n",
    "                    \"fiba\": 10, \"olympics\": 8, \"international\": 7, \"coachclinic\": 7, \"eurobasket\": 6,\n",
    "                    \"rebounddrill\": 5, \"pickandroll\": 5, \"zone\": 5, \"man2man\": 5, \"backboard\": 4\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"Music\": {\n",
    "        \"_words\": {\n",
    "            \"note\": 10, \"melody\": 9, \"tune\": 8, \"rhythm\": 7, \"harmony\": 6,\n",
    "            \"composition\": 5, \"instrument\": 5, \"rehearsal\": 5, \"performance\": 5, \"conductor\": 4\n",
    "        },\n",
    "        \"Rock\": {\n",
    "            \"_words\": {\n",
    "                \"guitar\": 10, \"band\": 9, \"amplifier\": 8, \"riff\": 7, \"drum\": 6,\n",
    "                \"bass\": 5, \"vocalist\": 5, \"solo\": 5, \"tour\": 5, \"album\": 4\n",
    "            },\n",
    "            \"Heavy Metal\": {\n",
    "                \"_words\": {\n",
    "                    \"metallica\": 10, \"megadeth\": 8, \"ironmaiden\": 7, \"blackmetal\": 7, \"distortion\": 6,\n",
    "                    \"headbang\": 5, \"thrash\": 5, \"riffage\": 5, \"growl\": 5, \"moshpit\": 4\n",
    "                }\n",
    "            },\n",
    "            \"Punk Rock\": {\n",
    "                \"_words\": {\n",
    "                    \"punk\": 10, \"ramones\": 8, \"anarchy\": 7, \"hardcore\": 7, \"mosh\": 6,\n",
    "                    \"diy\": 5, \"gutter\": 5, \"scene\": 5, \"fastbeat\": 5, \"underground\": 4\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"Jazz\": {\n",
    "            \"_words\": {\n",
    "                \"saxophone\": 10, \"improv\": 9, \"swing\": 8, \"bassline\": 7, \"piano\": 6,\n",
    "                \"trumpet\": 5, \"brushes\": 5, \"ensemble\": 5, \"standard\": 5, \"club\": 4\n",
    "            },\n",
    "            \"Bebop\": {\n",
    "                \"_words\": {\n",
    "                    \"charlieparker\": 10, \"dizzygillespie\": 8, \"fasttempo\": 7, \"complexharmony\": 7, \"jam\": 6,\n",
    "                    \"bebopscale\": 5, \"alteredchord\": 5, \"contrafact\": 5, \"sessions\": 5, \"birdland\": 4\n",
    "                }\n",
    "            },\n",
    "            \"Smooth Jazz\": {\n",
    "                \"_words\": {\n",
    "                    \"groove\": 10, \"softsax\": 8, \"fusionsound\": 7, \"laidback\": 7, \"lounge\": 6,\n",
    "                    \"chill\": 5, \"background\": 5, \"ambient\": 5, \"radio\": 5, \"crossover\": 4\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def get_children(topic_node):\n",
    "    \"\"\"Return the children topics of a given node (excluding the '_words' key).\"\"\"\n",
    "    return {k:v for k,v in topic_node.items() if k != \"_words\"}\n",
    "\n",
    "def is_leaf_topic(topic_node):\n",
    "    \"\"\"Check if the given node is a leaf (no children except '_words').\"\"\"\n",
    "    children = get_children(topic_node)\n",
    "    return len(children) == 0\n",
    "\n",
    "def sample_path(topic_node):\n",
    "    \"\"\"\n",
    "    Randomly sample a path from the given topic_node down to a leaf.\n",
    "    We move down the tree until we reach a leaf node.\n",
    "    \"\"\"\n",
    "    path = [topic_node]\n",
    "    current_node = topic_node\n",
    "\n",
    "    while True:\n",
    "        children = get_children(current_node)\n",
    "        if not children:\n",
    "            # Leaf node\n",
    "            break\n",
    "        # Randomly pick a child topic\n",
    "        child_topic = random.choice(list(children.keys()))\n",
    "        current_node = children[child_topic]\n",
    "        path.append(current_node)\n",
    "\n",
    "    return path\n",
    "\n",
    "def combine_word_distributions(path_nodes):\n",
    "    \"\"\"\n",
    "    Combine the word distributions from all nodes along the path.\n",
    "    Return a list of words and their corresponding probability distribution.\n",
    "    \"\"\"\n",
    "    combined_counts = {}\n",
    "    for node in path_nodes:\n",
    "        for w, c in node[\"_words\"].items():\n",
    "            combined_counts[w] = combined_counts.get(w, 0) + c\n",
    "\n",
    "    # Normalize to form a probability distribution\n",
    "    total = sum(combined_counts.values())\n",
    "    words = list(combined_counts.keys())\n",
    "    probs = [combined_counts[w] / total for w in words]\n",
    "    return words, probs\n",
    "\n",
    "def generate_document(topic_tree, num_words=100):\n",
    "    \"\"\"\n",
    "    Generate a single document.\n",
    "    1. Sample a path from the root to a leaf.\n",
    "    2. Combine word distributions along that path.\n",
    "    3. Sample words to form the document.\n",
    "    Return the document (list of words) and the path of topic node references.\n",
    "    \"\"\"\n",
    "    # Start from the root topic tree\n",
    "    path_nodes = sample_path(topic_tree)\n",
    "    words, probs = combine_word_distributions(path_nodes)\n",
    "    doc_words = np.random.choice(words, size=num_words, p=probs).tolist()\n",
    "    return doc_words, path_nodes\n",
    "\n",
    "def find_key_for_node(parent_node, child_node_ref):\n",
    "    \"\"\"Find the key in parent_node that corresponds to child_node_ref.\"\"\"\n",
    "    for k, v in parent_node.items():\n",
    "        if k == \"_words\":\n",
    "            continue\n",
    "        if v is child_node_ref:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "def get_path_labels(topic_tree, path_nodes):\n",
    "    \"\"\"\n",
    "    Given the path_nodes (which are references to nested dictionaries),\n",
    "    find the corresponding labels (keys) from the root to leaf.\n",
    "    \"\"\"\n",
    "    labels = [\"Root\"]  # The top-level node is root (unnamed), we label it \"Root\"\n",
    "    current_node = topic_tree\n",
    "    for node_ref in path_nodes[1:]:\n",
    "        key = find_key_for_node(current_node, node_ref)\n",
    "        labels.append(key)\n",
    "        current_node = node_ref\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for synthetic corpus generation\n",
    "num_documents = 50\n",
    "num_words_per_doc = 100\n",
    "\n",
    "documents = []\n",
    "document_paths = []\n",
    "for _ in range(num_documents):\n",
    "    doc_words, path_nodes = generate_document(topic_tree, num_words=num_words_per_doc)\n",
    "    documents.append(doc_words)\n",
    "    path_labels = get_path_labels(topic_tree, path_nodes)\n",
    "    document_paths.append(path_labels)\n",
    "\n",
    "# Store results to a JSON file for future use\n",
    "with open(\"synthetic_corpus_extended.json\", \"w\") as f:\n",
    "    json.dump({\"documents\": documents, \"document_paths\": document_paths}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data\n",
    "with open(\"synthetic_corpus_extended.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "documents = data[\"documents\"]\n",
    "document_paths = data[\"document_paths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gibbs sampling completed.\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from documents\n",
    "vocab = list(set(word for doc in documents for word in doc))\n",
    "\n",
    "# Initialize the nCRPTree instance\n",
    "gamma = 1.0\n",
    "eta = 0.1\n",
    "num_levels = 4 \n",
    "m = 0.5\n",
    "pi = 1.0\n",
    "tree = nCRPTree(gamma=gamma, eta=eta, num_levels=num_levels, vocab=vocab, m=m, pi=pi)\n",
    "\n",
    "# Run Gibbs sampling\n",
    "num_iterations = 2000\n",
    "tree.gibbs_sampling(documents, num_iterations=num_iterations, burn_in=100, thinning=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree visualization saved as predicted_hierarchy.png\n",
      "Actual hierarchy saved as actual_hierarchy.png\n"
     ]
    }
   ],
   "source": [
    "# 1. Visualize the predicted hierarchy from the hLDA model\n",
    "print_tree_graphviz(tree.root, vocab, filename='predicted_hierarchy', view=False)\n",
    "\n",
    "# 2. Visualize the actual hierarchy:\n",
    "def visualize_hierarchy_dict(hierarchy_dict, parent_id=None, graph=None, node_id_counter=None):\n",
    "    \"\"\"\n",
    "    Visualize the actual hierarchy defined by a dictionary (similar to topic_tree).\n",
    "    \"\"\"\n",
    "    if graph is None:\n",
    "        graph = Digraph(comment='Actual Hierarchy')\n",
    "        graph.attr('node', shape='box', style='filled', color='lightgreen')\n",
    "        node_id_counter = [0]\n",
    "    \n",
    "    current_id = node_id_counter[0]\n",
    "    node_id_counter[0] += 1\n",
    "\n",
    "    # Extract words for label\n",
    "    node_words = hierarchy_dict.get(\"_words\", {})\n",
    "    \n",
    "    # Just show some words to make the node meaningful\n",
    "    top_words = sorted(node_words.keys(), key=lambda w: node_words[w], reverse=True)[:3]\n",
    "    \n",
    "    # Attempt a label that shows a \"topic\" name if available (keys other than _words)\n",
    "    children_keys = [k for k in hierarchy_dict.keys() if k != \"_words\"]\n",
    "    label = \"Node\"\n",
    "    if parent_id is None:\n",
    "        label = \"Root\\nWords: \" + \", \".join(top_words)\n",
    "    else:\n",
    "        # The parent's children keys may give us a hint. \n",
    "        # But here we don't have direct parent-child name association stored, \n",
    "        # as we did in generation. We can just show words.\n",
    "        label = \"Words: \" + \", \".join(top_words)\n",
    "    graph.node(str(current_id), label=label)\n",
    "\n",
    "    # Add edge if not root\n",
    "    if parent_id is not None:\n",
    "        graph.edge(str(parent_id), str(current_id))\n",
    "\n",
    "    # Recursively traverse children\n",
    "    for child_key, child_node in hierarchy_dict.items():\n",
    "        if child_key == \"_words\":\n",
    "            continue\n",
    "        # Child is a sub-dictionary\n",
    "        graph, node_id_counter = visualize_hierarchy_dict(child_node, parent_id=current_id, graph=graph, node_id_counter=node_id_counter)\n",
    "\n",
    "    return graph, node_id_counter\n",
    "\n",
    "# Generate and save the actual hierarchy visualization\n",
    "actual_graph, _ = visualize_hierarchy_dict(topic_tree)\n",
    "actual_graph.render('actual_hierarchy', view=False, format='png')\n",
    "\n",
    "print(\"Actual hierarchy saved as actual_hierarchy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 23 path visualization saved as doc_23_path.png\n",
      "Document 19 path visualization saved as doc_19_path.png\n",
      "Document 15 path visualization saved as doc_15_path.png\n",
      "Selected document paths visualized. Check doc_<id>_path.png files.\n"
     ]
    }
   ],
   "source": [
    "doc_ids_to_visualize = random.sample(range(len(documents)), 3)\n",
    "\n",
    "for doc_id in doc_ids_to_visualize:\n",
    "    visualize_document_path(tree, doc_id, vocab, filename=f'doc_{doc_id}_path', view=False, top_n=5)\n",
    "\n",
    "print(\"Selected document paths visualized. Check doc_<id>_path.png files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA3288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
