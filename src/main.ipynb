{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from functools import lru_cache\n",
    "from scipy.special import gammaln\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical LDA (hLDA) Implementation Overview\n",
    "\n",
    "## 1. Overall Description\n",
    "\n",
    "This implementation of hierarchical LDA (hLDA) follows the model described in Blei, Griffiths, and Jordan (2010). The key idea is to model documents with a hierarchical topic structure, where each document chooses a path through a tree of topics, and each word in the document is assigned to a level along that path.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "- **Nested Chinese Restaurant Process (nCRP) Tree:**  \n",
    "  The hierarchy of topics is represented by a tree structure. Each node represents a topic, and child nodes represent subtopics. The nCRP defines a distribution over infinite trees, but in practice, we truncate at a finite number of levels.\n",
    "\n",
    "- **Document Paths:**  \n",
    "  Each document selects a path from the root down through the levels of the tree. This path indicates which set of topics are associated with the document at each hierarchical level.\n",
    "\n",
    "- **Word-Level Assignments:**  \n",
    "  Each word in the document is associated with a particular level along the chosen path, indicating the hierarchical depth at which that word is most suitably modeled.\n",
    "\n",
    "**Classes and Methodology:**\n",
    "\n",
    "- `Node`: Represents a single node (topic) in the tree. It stores word counts, document counts, and references to children.\n",
    "- `nCRPTree`: Manages the entire hierarchical model:\n",
    "  - Maintains the tree and paths.\n",
    "  - Stores per-document level assignments and words.\n",
    "  - Implements methods for path and level sampling according to hLDA.\n",
    "\n",
    "The overall approach is a **collapsed Gibbs sampling** algorithm, integrating out the multinomial distributions. We repeatedly sample:\n",
    "1. The path for each document.\n",
    "2. The level assignment for each word in each document.\n",
    "\n",
    "By iterating these steps, we approximate samples from the posterior distribution of the latent structure defined by the hLDA model.\n",
    "\n",
    "## 2. Initialization\n",
    "\n",
    "Before sampling, we need to:\n",
    "1. Initialize a root node and set `num_levels` for the hierarchy.\n",
    "2. For each document, sample an initial path from the root down to `num_levels` levels using the nCRP prior.\n",
    "3. Assign each word in the document to an initial level along that path (often randomly).\n",
    "\n",
    "This initialization provides a starting point for the Gibbs sampler.\n",
    "\n",
    "## 3. Level Sampling\n",
    "\n",
    "Level assignment involves sampling the latent variable $ z_{d,n}$, the level of word $n$ in document \\( d \\).\n",
    "\n",
    "After removing the current assignment of a word, we compute:\n",
    "\n",
    "$$\n",
    "p(z_{d,n} = k \\mid z_{-(d,n)}, w, m, \\pi, \\eta) \\propto p(z_{d,n} = k \\mid z_{d,-n}, m, \\pi) \\times p(w_{d,n} \\mid z, c, w_{-(d,n)}, \\eta).\n",
    "$$\n",
    "\n",
    "**Prior term:**\n",
    "\n",
    "For levels $k \\leq \\max(z_{d,-n})$, the prior is given by the stick-breaking construction:\n",
    "\n",
    "$$\n",
    "p(z_{d,n}=k \\mid z_{d,-n}, m, \\pi) = \\frac{m\\pi + \\# [z_{d,-n}=k]}{\\pi + \\# [z_{d,-n}\\geq k]} \\prod_{j=1}^{k-1} \\frac{(1 - m)\\pi + \\# [z_{d,-n}>j]}{\\pi + \\# [z_{d,-n}\\geq j]}.\n",
    "$$\n",
    "\n",
    "If $ k > \\max(z_{d,-n})$, we consider going deeper into the hierarchy. This involves a series of Bernoulli trials as described in the paper: at each new level, we decide whether to \"stop\" and place the word there, or try to go even deeper.\n",
    "\n",
    "**Likelihood term:**\n",
    "\n",
    "The likelihood of placing word $w_{d,n}$ at level $k$ (in the node corresponding to that level on the document’s path) is computed using the Dirichlet-multinomial integral. After removing the current word, the probability (up to a normalization constant) is:\n",
    "\n",
    "$$\n",
    "p(w_{d,n}\\mid ...) \\propto \\eta + \\# [w_{d,n}\\text{ in that node}] \n",
    "$$\n",
    "\n",
    "and normalized by the total words plus $\\eta$ times the vocabulary size.\n",
    "\n",
    "**Process:**\n",
    "1. Remove the word’s old assignment.\n",
    "2. Compute the prior for all feasible levels.\n",
    "3. Compute the likelihood for placing the word at each level.\n",
    "4. Sample a new level from the normalized distribution.\n",
    "5. Update counts accordingly.\n",
    "\n",
    "Level sampling ensures that each word finds a suitable \"depth\" in the topic hierarchy that best explains it, given the structure defined by other words and documents.\n",
    "\n",
    "## 4. Path Sampling\n",
    "\n",
    "Path sampling involves sampling the path $c_{d}$ for each document $d$. The path defines which topics (nodes) the document uses at each level.\n",
    "\n",
    "We sample:\n",
    "\n",
    "$$\n",
    "p(c_d \\mid w_d, c_{-d}, z, \\eta, \\gamma) \\propto p(c_d \\mid c_{-d}, \\gamma) \\times p(w_d \\mid c, w_{-d}, z, \\eta).\n",
    "$$\n",
    "\n",
    "**nCRP Prior:**\n",
    "\n",
    "The nCRP defines a distribution over infinite trees. When choosing a child node at each level, we have:\n",
    "\n",
    "- Probability of an existing child:\n",
    "  $$\n",
    "  \\frac{\\text{child.documents}}{\\text{parent.documents} + \\gamma}\n",
    "  $$\n",
    "- Probability of creating a new child node:\n",
    "  $$\n",
    "  \\frac{\\gamma}{\\text{parent.documents} + \\gamma}.\n",
    "  $$\n",
    "\n",
    "This ensures that popular paths (with many documents) are chosen more often, but there is always a chance to create new branches, fostering hierarchical growth.\n",
    "\n",
    "**Likelihood:**\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w}_d \\mid \\mathbf{c}, \\mathbf{w}_{-d}, \\mathbf{z}, \\eta) =\n",
    "\\prod_{\\ell=1}^{\\max(\\mathbf{z}_d)} \n",
    "\\frac{\\Gamma \\left( \\sum_w \\left[ \\#\\big[\\mathbf{z}_{-d} = \\ell, \\mathbf{c}_{-d, \\ell} = c_{d, \\ell}, \\mathbf{w}_{-d} = w \\big] \\right] + V \\eta \\right)}\n",
    "{\\prod_w \\Gamma \\left( \\#\\big[\\mathbf{z}_{-d} = \\ell, \\mathbf{c}_{-d, \\ell} = c_{d, \\ell}, \\mathbf{w}_{-d} = w \\big] + \\eta \\right)}\n",
    "\\cdot\n",
    "\\frac{\\prod_w \\Gamma \\left( \\#\\big[\\mathbf{z} = \\ell, \\mathbf{c}_\\ell = c_{d, \\ell}, \\mathbf{w} = w \\big] + \\eta \\right)}\n",
    "{\\Gamma \\left( \\sum_w \\left[ \\#\\big[\\mathbf{z} = \\ell, \\mathbf{c}_\\ell = c_{d, \\ell}, \\mathbf{w} = w \\big] \\right] + V \\eta \\right)},\n",
    "$$\n",
    "\n",
    "For a given path, the integrated likelihood $p(w_d \\mid c, w_{-d}, z, \\eta)$ is computed level-by-level. Using gamma functions, we integrate out the topic-word distributions to get a Dirichlet-multinomial probability. This step involves ratios of gamma functions of the word counts at each node along the path.\n",
    "\n",
    "**Sampling Procedure:**\n",
    "1. Remove the document from the tree (leave-one-out counts).\n",
    "2. For each level from the root to the maximum depth, sample which child topic to choose, considering both existing children and a potential new node.\n",
    "3. This top-down approach avoids enumerating all possible paths.\n",
    "4. After deciding the entire path, add the document back with updated counts.\n",
    "\n",
    "By repeatedly sampling document paths, we refine the hierarchical structure, allowing popular and useful branches to grow, while pruning or leaving underused branches less populated.\n",
    "\n",
    "## 5. Gibbs Sampling, Burn-in, and Thinning\n",
    "\n",
    "**Gibbs Sampling Iterations:**\n",
    "\n",
    "1. **Initialization:** Assign each document a path and assign each word a level at random.\n",
    "2. **Iterate:**\n",
    "   - For each document:\n",
    "     - Sample the document’s path given the current state of the tree.\n",
    "     - Sample the level assignment for each word in that document.\n",
    "   - Move to the next iteration.\n",
    "\n",
    "**Burn-in:**\n",
    "- The first several iterations might not reflect the stationary distribution.\n",
    "- We use a burn-in period (e.g., 100 iterations) to discard these early samples, ensuring that we only collect samples when the chain is \"well-mixed.\"\n",
    "\n",
    "**Thinning:**\n",
    "- Samples might be autocorrelated. To reduce correlation, we only keep every \\(N\\)-th sample (e.g., every 10th iteration) after burn-in.\n",
    "- This provides a set of approximately independent samples from the posterior.\n",
    "\n",
    "## 6. Visualization\n",
    "\n",
    "To understand the learned hierarchy, we visualize the resulting topic tree:\n",
    "\n",
    "- Each node is represented with information such as the number of documents and top words.\n",
    "- Edges indicate the hierarchical structure (parent-child relationships).\n",
    "- Tools like Graphviz can render a tree diagram to show how general topics branch into more specific subtopics at deeper levels.\n",
    "\n",
    "This visualization can help interpret the hierarchical topic structure discovered by hLDA, identifying general themes at the root and more specialized ones at lower levels.\n",
    "\n",
    "In addition to visualizing the entire tree, we can also extract the specific path that a given document follows through the hierarchy. Each document in hLDA is associated with a single path from the root node down to a node at the specified `num_levels`. This path shows which sequence of topics the document \"chooses\" at increasing levels of specificity.\n",
    "\n",
    "By visualizing just the path of a single document, we can:\n",
    "\n",
    "- Understand how the document is positioned in the topic hierarchy.\n",
    "- See which general topics (at higher levels) and which more specific subtopics (at lower levels) are used by the document.\n",
    "- Inspect top words at each node along the document’s path.\n",
    "\n",
    "**Proposed Functionality:**\n",
    "\n",
    "We introduce a function `visualize_document_path` that:\n",
    "\n",
    "1. Takes the `tree` (an `nCRPTree` object), a `doc_id`, the `vocab`, and optional parameters like `filename` and `view`.\n",
    "2. Extracts the path nodes for that document from `tree.paths[doc_id]`.\n",
    "3. Uses Graphviz to create a small directed graph containing just those nodes.\n",
    "4. Labels each node in the path with its level, document counts, and top words.\n",
    "5. Connects the nodes in a linear fashion (root → level 1 → level 2 → ...), since a document’s path is a simple chain down the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Represents a node in a hierarchical topic tree.\n",
    "\n",
    "    Attributes:\n",
    "        children (dict): A dictionary mapping topic IDs to child Node instances.\n",
    "        documents (int): Number of documents passing through this node.\n",
    "        word_counts (defaultdict): A dictionary counting the occurrences of each word in this node.\n",
    "        total_words (int): Total number of non-unique words in this node.\n",
    "        parent (Node, optional): Reference to the parent Node. Defaults to None.\n",
    "        level (int): The depth level of this node in the tree. Root node has level 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent=None, level=0):\n",
    "        \"\"\"\n",
    "        Initializes a new Node instance.\n",
    "\n",
    "        Args:\n",
    "            parent (Node, optional): The parent node in the hierarchy. Defaults to None.\n",
    "            level (int, optional): The depth level of the node in the tree. Root node has level 0. Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.children = {} \n",
    "        self.documents = 0  \n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.total_words = 0 \n",
    "        self.parent = parent\n",
    "        self.level = level\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Determines whether the node is a leaf node (i.e., has no children).\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the node has no children, False otherwise.\n",
    "        \"\"\"\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def add_child(self, topic_id):\n",
    "        \"\"\"\n",
    "        Adds a child node with the specified topic ID to the current node.\n",
    "\n",
    "        Args:\n",
    "            topic_id (hashable): The identifier for the child topic.\n",
    "\n",
    "        Returns:\n",
    "            Node: The newly created child Node instance.\n",
    "        \"\"\"\n",
    "        child_node = Node(parent=self, level=self.level + 1)\n",
    "        self.children[topic_id] = child_node\n",
    "        return child_node\n",
    "\n",
    "    def remove_child(self, topic_id):\n",
    "        \"\"\"\n",
    "        Removes the child node with the specified topic ID from the current node.\n",
    "\n",
    "        Args:\n",
    "            topic_id (hashable): The identifier of the child topic to remove.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If the specified topic_id does not exist among the children.\n",
    "        \"\"\"\n",
    "        if topic_id in self.children:\n",
    "            del self.children[topic_id]\n",
    "        else:\n",
    "            raise KeyError(f\"Topic ID {topic_id} not found among the children.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nCRPTree:\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        root (Node): The root node of the nCRP tree.\n",
    "        gamma (float): Concentration parameter for the nCRP, controlling the likelihood of creating new topics.\n",
    "        eta (float): Smoothing parameter for topic-word distributions.\n",
    "        V (int): Vocabulary size.\n",
    "        vocab (list): List of vocabulary words.\n",
    "        eta_sum (float): Precomputed sum of eta multiplied by vocabulary size for efficiency.\n",
    "        num_levels (int): Maximum depth of the hierarchical tree.\n",
    "        paths (dict): Maps document IDs to their assigned path of nodes in the tree.\n",
    "        levels (dict): Maps document IDs to their word-level assignments.\n",
    "        document_words (dict): Maps document IDs to their preprocessed word lists.\n",
    "        m (float): Hyperparameter influencing level assignments.\n",
    "        pi (float): Hyperparameter influencing level assignments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma, eta, num_levels, vocab, m=0.5, pi=1.0):\n",
    "        \"\"\"\n",
    "        Initializes the nCRP tree with specified hyperparameters and vocabulary.\n",
    "\n",
    "        Args:\n",
    "            gamma (float): Concentration parameter for the nCRP.\n",
    "            eta (float): Smoothing parameter for topic-word distributions.\n",
    "            num_levels (int): Maximum depth of the hierarchical tree.\n",
    "            vocab (list): List of vocabulary words of the entire corpus\n",
    "            m (float, optional): Hyperparameter influencing level assignments. Defaults to 0.5.\n",
    "            pi (float, optional): Hyperparameter influencing level assignments. Defaults to 1.0.\n",
    "        \"\"\"\n",
    "        self.root = Node()\n",
    "        self.gamma = gamma\n",
    "        self.eta = eta\n",
    "        self.V = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.eta_sum = self.eta * self.V\n",
    "        self.num_levels = num_levels\n",
    "        self.paths = {}\n",
    "        self.levels = {}               \n",
    "        self.document_words = {}        ## Pre-processed vacabulary for each of the document.\n",
    "        self.m = m\n",
    "        self.pi = pi\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_gammaln(self, x):\n",
    "        \"\"\"\n",
    "        Caches the computation of the logarithm of the gamma function for efficiency.\n",
    "\n",
    "        Args:\n",
    "            x (float): The input value for which to compute gammaln(x).\n",
    "\n",
    "        Returns:\n",
    "            float: The computed value of gammaln(x).\n",
    "        \"\"\"\n",
    "        return gammaln(x)\n",
    "\n",
    "    def sample_ncrp_path(self, node):\n",
    "        \"\"\"\n",
    "        Samples a path (topic) for a new document based on the current node's children.\n",
    "\n",
    "        It calculates the probability of assigning the document to each existing child topic\n",
    "        and the probability of creating a new topic. It then samples a topic based on these probabilities.\n",
    "\n",
    "        Args:\n",
    "            node (Node): The current node from which to sample the next topic in the path.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                chosen (hashable): The chosen topic ID (existing or new).\n",
    "                is_new (bool): Flag indicating whether a new topic was created.\n",
    "        \"\"\"\n",
    "        total_customers = node.documents\n",
    "        topic_probabilities = {}\n",
    "\n",
    "        # Existing children probabilities\n",
    "        for topic_id, child in node.children.items():\n",
    "            topic_probabilities[topic_id] = child.documents / (total_customers + self.gamma)\n",
    "\n",
    "        # New child probability\n",
    "        new_topic_key = max(node.children.keys(), default=0) + 1\n",
    "        topic_probabilities[new_topic_key] = self.gamma / (total_customers + self.gamma)\n",
    "\n",
    "        topics = list(topic_probabilities.keys())\n",
    "        probs = np.array(list(topic_probabilities.values()))\n",
    "        probs /= probs.sum()\n",
    "        chosen = np.random.choice(topics, p=probs)\n",
    "        is_new = (chosen not in node.children)\n",
    "        return chosen, is_new\n",
    "\n",
    "    def initialize_new_path(self, max_depth, document_id):\n",
    "        \"\"\"\n",
    "        Initializes a new path for a document up to the specified maximum depth.\n",
    "\n",
    "        It assigns the document to a path by sampling topics at each level and updating\n",
    "        document counts for the nodes along the path.\n",
    "\n",
    "        Args:\n",
    "            max_depth (int): The maximum depth to assign the document in the tree.\n",
    "            document_id (int): The unique identifier for the document being added.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of Node instances representing the path assigned to the document.\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        current_node.documents += 1\n",
    "        path_nodes = [current_node]\n",
    "\n",
    "        for level in range(1, max_depth):\n",
    "            topic_id, is_new = self.sample_ncrp_path(current_node)\n",
    "            if is_new:\n",
    "                child_node = current_node.add_child(topic_id)\n",
    "            else:\n",
    "                child_node = current_node.children[topic_id]\n",
    "            child_node.documents += 1\n",
    "            path_nodes.append(child_node)\n",
    "            current_node = child_node\n",
    "\n",
    "        self.paths[document_id] = path_nodes\n",
    "        return path_nodes\n",
    "\n",
    "    def initialise_tree(self, corpus, max_depth):\n",
    "        \"\"\"\n",
    "        Initializes the nCRP tree by assigning each document in the corpus to a path.\n",
    "\n",
    "        For each document, it assigns a path through the tree and assigns words to levels\n",
    "        within that path based on random sampling.\n",
    "\n",
    "        Args:\n",
    "            corpus (list of list of str): The preprocessed corpus where each document is a list of words.\n",
    "            max_depth (int): The maximum depth to assign in the tree.\n",
    "        \"\"\"\n",
    "        for doc_id, doc_words in enumerate(corpus):\n",
    "            self.document_words[doc_id] = doc_words\n",
    "            \n",
    "            # Path assignment\n",
    "            path_nodes = self.initialize_new_path(max_depth, doc_id)\n",
    "            doc_levels = []\n",
    "            num_levels = len(path_nodes)\n",
    "            \n",
    "            # Level assignment\n",
    "            for w in doc_words: \n",
    "                level = np.random.randint(0, num_levels)\n",
    "                doc_levels.append(level)\n",
    "                node = path_nodes[level]\n",
    "                node.word_counts[w] += 1\n",
    "                node.total_words += 1\n",
    "            self.levels[doc_id] = doc_levels\n",
    "\n",
    "    def add_document(self, document_id, path_nodes, level_word_counts):\n",
    "        \"\"\"\n",
    "        Adds a document to the tree by updating document paths and word counts.\n",
    "\n",
    "        Args:\n",
    "            document_id (int): The unique identifier for the document being added.\n",
    "            path_nodes (list of Node): The path of nodes assigned to the document.\n",
    "            level_word_counts (dict): A mapping from level indices to word count dictionaries.\n",
    "        \"\"\"\n",
    "        self.paths[document_id] = path_nodes\n",
    "        for node in path_nodes:\n",
    "            node.documents += 1\n",
    "        for level, w_counts in level_word_counts.items():\n",
    "            node = path_nodes[level]\n",
    "            for w, cnt in w_counts.items():\n",
    "                node.word_counts[w] += cnt\n",
    "                node.total_words += cnt\n",
    "\n",
    "    def remove_document(self, document_id):\n",
    "        \"\"\"\n",
    "        Removes a document from the tree, updating document counts and pruning empty nodes.\n",
    "\n",
    "        Args:\n",
    "            document_id (int): The unique identifier for the document being removed.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If the document ID does not exist in the tree.\n",
    "        \"\"\"\n",
    "        if document_id not in self.paths:\n",
    "            return\n",
    "        path_nodes = self.paths[document_id]\n",
    "        doc_levels = self.levels[document_id]\n",
    "        doc_words = self.document_words[document_id]\n",
    "\n",
    "        # Decrement word counts\n",
    "        for w, lvl in zip(doc_words, doc_levels):\n",
    "            node = path_nodes[lvl]\n",
    "            node.word_counts[w] -= 1\n",
    "            if node.word_counts[w] == 0:\n",
    "                del node.word_counts[w]\n",
    "            node.total_words -= 1\n",
    "\n",
    "        # Decrement document counts\n",
    "        for node in path_nodes:\n",
    "            node.documents -= 1\n",
    "\n",
    "        # Prune empty leaves\n",
    "        for node in reversed(path_nodes):\n",
    "            if node.documents == 0 and node.is_leaf() and node.parent is not None:\n",
    "                parent = node.parent\n",
    "                remove_id = None\n",
    "                for tid, cnode in parent.children.items():\n",
    "                    if cnode == node:\n",
    "                        remove_id = tid\n",
    "                        break\n",
    "                if remove_id is not None:\n",
    "                    parent.remove_child(remove_id)\n",
    "\n",
    "        del self.paths[document_id]\n",
    "        del self.levels[document_id]\n",
    "        del self.document_words[document_id]\n",
    "        \n",
    "    \n",
    "    # Sampling Paths\n",
    "    def get_level_word_counts(self, document_words, document_levels):\n",
    "        \"\"\"\n",
    "        Aggregates word counts for each level in a document.\n",
    "\n",
    "        Args:\n",
    "            document_words (list of str): The list of words in the document.\n",
    "            document_levels (list of int): The list of level assignments for each word.\n",
    "\n",
    "        Returns:\n",
    "            dict: A mapping from level indices to dictionaries of word counts.\n",
    "        \"\"\"\n",
    "        level_word_counts = {}\n",
    "        for w, lvl in zip(document_words, document_levels):\n",
    "            if lvl not in level_word_counts:\n",
    "                level_word_counts[lvl] = {}\n",
    "            level_word_counts[lvl][w] = level_word_counts[lvl].get(w, 0) + 1\n",
    "        return level_word_counts\n",
    "\n",
    "    def level_likelihood(self, node, M): \n",
    "        \"\"\"\n",
    "        Computes the log-likelihood of words at a given level in a node.\n",
    "\n",
    "        Args:\n",
    "            node (Node): The node representing the current topic.\n",
    "            M (dict): A dictionary of word counts assigned to this level.\n",
    "\n",
    "        Returns:\n",
    "            float: The computed log-likelihood.\n",
    "        \"\"\"\n",
    "        Nminus = node.word_counts # The current counts of words in the node (excluding the new assignments).\n",
    "        sumNminus = node.total_words # Total number of words in the node before adding the new assignments.\n",
    "        sumM = sum(M.values()) # Total number of new word assignments.\n",
    "        sumN = sumNminus + sumM # Updated total word count after adding new assignments.\n",
    "\n",
    "        log_part1 = self.cached_gammaln(sumNminus + self.eta_sum)\n",
    "        for w_count in Nminus.values():\n",
    "            log_part1 -= self.cached_gammaln(w_count + self.eta)\n",
    "\n",
    "        log_part2 = 0.0\n",
    "        for w, n_w_minus in Nminus.items():\n",
    "            n_w = n_w_minus + M.get(w, 0)\n",
    "            log_part2 += self.cached_gammaln(n_w + self.eta)\n",
    "        for w, mcount in M.items():\n",
    "            if w not in Nminus:\n",
    "                log_part2 += self.cached_gammaln(mcount + self.eta)\n",
    "\n",
    "        log_part2 -= self.cached_gammaln(sumN + self.eta_sum)\n",
    "\n",
    "        return log_part1 + log_part2\n",
    "\n",
    "    def level_prior(self, parent, child_is_new, child_node=None):\n",
    "        \"\"\"\n",
    "        Computes the log prior probability for assigning a child node.\n",
    "\n",
    "        Args:\n",
    "            parent (Node): The parent node in the hierarchy.\n",
    "            child_is_new (bool): Flag indicating whether the child node is new.\n",
    "            child_node (Node, optional): The child node instance. Required if child_is_new is False. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            float: The computed log prior probability.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If child_is_new is False and child_node is not provided.\n",
    "        \"\"\"\n",
    "        total_customers = parent.documents\n",
    "        if child_is_new:\n",
    "            return log(self.gamma) - log(total_customers + self.gamma)\n",
    "        else:\n",
    "            return log(child_node.documents) - log(total_customers + self.gamma)\n",
    "\n",
    "    def sample_path_level(self, parent_node, level_word_counts, level_index):\n",
    "        \"\"\"\n",
    "        Samples a topic for a specific level in the path based on prior and likelihood.\n",
    "\n",
    "        Args:\n",
    "            parent_node (Node): The parent node at the current level.\n",
    "            level_word_counts (dict): Word counts assigned to the current level.\n",
    "            level_index (int): The current level index in the path.\n",
    "\n",
    "        Returns:\n",
    "            Node: The child node assigned to the current level.\n",
    "        \"\"\"\n",
    "        candidates = list(parent_node.children.items())  # (topic_id, node)\n",
    "        new_topic_id = max(parent_node.children.keys(), default=0) + 1\n",
    "\n",
    "        M = level_word_counts.get(level_index, {})\n",
    "\n",
    "        # Existing children\n",
    "        log_probs = []\n",
    "        for topic_id, child_node in candidates:\n",
    "            lp = self.level_prior(parent_node, False, child_node)\n",
    "            lp += self.level_likelihood(child_node, M)\n",
    "            log_probs.append((lp, topic_id, False))\n",
    "\n",
    "        # New child\n",
    "        fake_node = Node(parent=parent_node, level=parent_node.level + 1)\n",
    "        lp_new = self.level_prior(parent_node, True)\n",
    "        lp_new += self.level_likelihood(fake_node, M)\n",
    "        log_probs.append((lp_new, new_topic_id, True))\n",
    "\n",
    "        lps = [x[0] for x in log_probs]\n",
    "        max_lp = max(lps)\n",
    "        weights = np.exp([lp - max_lp for lp in lps])\n",
    "        probs = weights / weights.sum()\n",
    "\n",
    "        chosen_index = np.random.choice(len(probs), p=probs)\n",
    "        chosen_lp, chosen_topic_id, chosen_is_new = log_probs[chosen_index]\n",
    "\n",
    "        if chosen_is_new:\n",
    "            child_node = parent_node.add_child(chosen_topic_id)\n",
    "        else:\n",
    "            child_node = parent_node.children[chosen_topic_id]\n",
    "\n",
    "        return child_node\n",
    "\n",
    "    def sample_path(self, document_id, document_words, document_levels):\n",
    "        \"\"\"\n",
    "        Samples a new path for a document, reassigning its path in the tree.\n",
    "\n",
    "        It removes the document from its current path, samples a new path based on word assignments,\n",
    "        and updates the tree accordingly.\n",
    "\n",
    "        Args:\n",
    "            document_id (int): The unique identifier for the document being sampled.\n",
    "            document_words (list of str): The list of words in the document.\n",
    "            document_levels (list of int): The list of level assignments for each word in the document.\n",
    "        \"\"\"\n",
    "        if document_id in self.paths:\n",
    "            self.remove_document(document_id)\n",
    "\n",
    "        level_word_counts = self.get_level_word_counts(document_words, document_levels)\n",
    "        current_node = self.root\n",
    "        path_nodes = [current_node]\n",
    "\n",
    "        for ell in range(1, self.num_levels):\n",
    "            child_node = self.sample_path_level(current_node, level_word_counts, ell)\n",
    "            path_nodes.append(child_node)\n",
    "            current_node = child_node\n",
    "\n",
    "        self.add_document(document_id, path_nodes, level_word_counts)\n",
    "        self.levels[document_id] = document_levels\n",
    "        self.document_words[document_id] = document_words\n",
    "        max_level = max(document_levels) if document_levels else 0\n",
    "        assert max_level < self.num_levels, \"Document level assignments exceed maximum tree depth.\"\n",
    "\n",
    "    ## Sampling Levels\n",
    "    def compute_level_prior_probs(self, z_counts, max_z, m, pi):\n",
    "        \"\"\"\n",
    "        Computes prior probabilities for levels based on current assignments.\n",
    "\n",
    "        Args:\n",
    "            z_counts (list of int): Counts of assignments at each level up to max_z.\n",
    "            max_z (int): The maximum level index with assignments.\n",
    "            m (float): Hyperparameter influencing level assignments.\n",
    "            pi (float): Hyperparameter influencing level assignments.\n",
    "\n",
    "        Returns:\n",
    "            list of float: The computed prior probabilities for each level.\n",
    "        \"\"\"\n",
    "        sum_ge = [0]*(max_z+1)\n",
    "        running_sum = 0\n",
    "        for level_idx in range(max_z, -1, -1):\n",
    "            running_sum += z_counts[level_idx]\n",
    "            sum_ge[level_idx] = running_sum\n",
    "\n",
    "        probs = []\n",
    "        for k in range(max_z+1):\n",
    "            numerator = (m * pi + z_counts[k])\n",
    "            denominator = (pi + sum_ge[k]) if sum_ge[k] > 0 else pi\n",
    "            level_prob = numerator / denominator\n",
    "\n",
    "            for j in range(k):\n",
    "                numerator_j = ((1 - m) * pi + z_counts[j])\n",
    "                denominator_j = (pi + sum_ge[j]) if sum_ge[j] > 0 else pi\n",
    "                level_prob *= (numerator_j / denominator_j)\n",
    "            probs.append(level_prob)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def sample_level_assignment_for_word(self, document_id, n):\n",
    "        \"\"\"\n",
    "        Samples a new level assignment for the nth word in a document.\n",
    "\n",
    "        It updates the word counts and level assignments accordingly.\n",
    "\n",
    "        Args:\n",
    "            document_id (int): The unique identifier for the document.\n",
    "            n (int): The index of the word in the document to resample.\n",
    "        \"\"\"\n",
    "        doc_words = self.document_words[document_id]\n",
    "        doc_levels = self.levels[document_id]\n",
    "        old_level = doc_levels[n]\n",
    "        w = doc_words[n]\n",
    "\n",
    "        path_nodes = self.paths[document_id]\n",
    "        old_node = path_nodes[old_level]\n",
    "        old_node.word_counts[w] -= 1\n",
    "        \n",
    "        # Pruning\n",
    "        if old_node.word_counts[w] == 0:\n",
    "            del old_node.word_counts[w] \n",
    "        old_node.total_words -= 1\n",
    "\n",
    "        # Marking word as unassigned by setting its level to -1.\n",
    "        doc_levels[n] = -1\n",
    "        \n",
    "        # Counts the number of words assigned to each level in the document, excluding the word being resampled.\n",
    "        z_counts = defaultdict(int)\n",
    "        for lvl in doc_levels:\n",
    "            if lvl >= 0:\n",
    "                z_counts[lvl] += 1\n",
    "\n",
    "        # Determine Maximum Current Level \n",
    "        max_z = max(z_counts.keys()) if z_counts else 0\n",
    "        \n",
    "        # Prior distribution over levels based on current assignments and hyperparameters\n",
    "        level_range = list(range(max_z + 1))\n",
    "        z_counts_list = [z_counts.get(k, 0) for k in level_range]\n",
    "        prior_probs = self.compute_level_prior_probs(z_counts_list, max_z, self.m, self.pi)\n",
    "\n",
    "        # Word likelihood for existing levels\n",
    "        word_likelihoods = []\n",
    "        for k in level_range:\n",
    "            node = path_nodes[k]\n",
    "            w_count = node.word_counts.get(w, 0)\n",
    "            likelihood = (w_count + self.eta) / (node.total_words + self.eta_sum)\n",
    "            word_likelihoods.append(likelihood)\n",
    "\n",
    "        for i in range(len(prior_probs)):\n",
    "            prior_probs[i] *= word_likelihoods[i]\n",
    "\n",
    "        # The remaining probability mass, representing the possibility of assigning the word to a deeper level beyond max_z.\n",
    "        sum_existing = sum(prior_probs)\n",
    "        leftover = 1.0 - sum_existing\n",
    "        final_levels = level_range[:]\n",
    "        final_probs = prior_probs[:]\n",
    "\n",
    "        # Consider going beyond max_z as per eq (3)\n",
    "        if leftover > 1e-15:\n",
    "            ell = max_z + 1\n",
    "            p_w_new = self.eta / self.eta_sum\n",
    "            chosen_level = None\n",
    "            while ell < self.num_levels:\n",
    "                p_success = (1 - self.m)*p_w_new\n",
    "                # Bernoulli trial:\n",
    "                success = (np.random.rand() < p_success)\n",
    "                if success:\n",
    "                    chosen_level = ell\n",
    "                    break\n",
    "                else:\n",
    "                    ell += 1\n",
    "\n",
    "            if chosen_level is None:\n",
    "                # If we fail all the way, assign the deepest level:\n",
    "                chosen_level = self.num_levels - 1\n",
    "\n",
    "            new_level = chosen_level\n",
    "        else:\n",
    "            # Just choose from existing levels\n",
    "            total = sum_existing\n",
    "            if total == 0:\n",
    "                # rare fallback\n",
    "                new_level = max_z\n",
    "            else:\n",
    "                probs = [p/total for p in final_probs]\n",
    "                new_level = np.random.choice(final_levels, p=probs)\n",
    "\n",
    "        # Update counts\n",
    "        new_node = path_nodes[new_level]\n",
    "        new_node.word_counts[w] = new_node.word_counts.get(w,0) + 1\n",
    "        new_node.total_words += 1\n",
    "        doc_levels[n] = new_level\n",
    "\n",
    "    def sample_levels_for_document(self, document_id):\n",
    "        \"\"\"\n",
    "        Samples new level assignments for all words in a document.\n",
    "\n",
    "        It iterates through each word in the document and resamples its level assignment.\n",
    "\n",
    "        Args:\n",
    "            document_id (int): The unique identifier for the document.\n",
    "        \"\"\"\n",
    "        doc_words = self.document_words[document_id]\n",
    "        for n in range(len(doc_words)):\n",
    "            self.sample_level_assignment_for_word(document_id, n)\n",
    "\n",
    "    def gibbs_sampling(self, corpus, num_iterations, burn_in=100, thinning=10):\n",
    "        \"\"\"\n",
    "        Performs Gibbs sampling to infer topic assignments and update the nCRP tree.\n",
    "\n",
    "        The method iteratively samples path assignments and word level assignments for each document\n",
    "        in the corpus, updating the tree structure accordingly. It also handles burn-in and thinning\n",
    "        periods to ensure proper convergence.\n",
    "\n",
    "        Args:\n",
    "            corpus (list of list of str): The preprocessed corpus where each document is a list of words.\n",
    "            num_iterations (int): Total number of Gibbs sampling iterations to perform.\n",
    "            burn_in (int, optional): Number of initial iterations to discard as burn-in. Defaults to 100.\n",
    "            thinning (int, optional): Interval for collecting samples (i.e., collect a sample every 'thinning' iterations). Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.initialise_tree(corpus, max_depth=self.num_levels)\n",
    "        for it in range(num_iterations):\n",
    "            for doc_id in range(len(corpus)):\n",
    "                document_words = self.document_words[doc_id]\n",
    "                document_levels = self.levels[doc_id]\n",
    "                self.sample_path(doc_id, document_words, document_levels)\n",
    "                self.sample_levels_for_document(doc_id)\n",
    "        print(\"Gibbs sampling completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(node, vocab, top_n=5):\n",
    "    word_counts = list(node.word_counts.items())\n",
    "    word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_words = [w for w, count in word_counts[:top_n]]\n",
    "    return top_words\n",
    "\n",
    "def print_tree(node, vocab, level=0):\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}Level {node.level}: docs={node.documents}, total_words={node.total_words}\")\n",
    "    top_words = get_top_words(node, vocab, top_n=5)\n",
    "    print(f\"{indent}  Top words: {top_words}\")\n",
    "    for child_id, child_node in node.children.items():\n",
    "        print_tree(child_node, vocab, level+1)\n",
    "\n",
    "def print_document_assignments(tree, doc_id):\n",
    "    doc_words = tree.document_words[doc_id]\n",
    "    doc_levels = tree.levels[doc_id]\n",
    "    print(f\"Document {doc_id}:\")\n",
    "    for w, lvl in zip(doc_words, doc_levels):\n",
    "        print(f\"  {w} -> level {lvl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree_graphviz(node, vocab, graph=None, parent_id=None, node_id_counter=None, label_map=None):\n",
    "    \"\"\"\n",
    "    Recursively traverse the tree and add nodes and edges to the Graphviz Digraph.\n",
    "\n",
    "    Args:\n",
    "        node (Node): The current node to visualize.\n",
    "        vocab (list): List of vocabulary words.\n",
    "        graph (Digraph, optional): The Graphviz Digraph object. Defaults to None.\n",
    "        parent_id (int, optional): The ID of the parent node. Defaults to None.\n",
    "        node_id_counter (list, optional): A single-element list acting as a mutable counter for node IDs. Defaults to None.\n",
    "        label_map (dict, optional): Mapping from node IDs to labels. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (graph, current_node_id, label_map)\n",
    "    \"\"\"\n",
    "    if graph is None:\n",
    "        graph = Digraph(comment='nCRP Tree')\n",
    "        graph.attr('node', shape='box', style='filled', color='lightblue')\n",
    "        label_map = {}\n",
    "    \n",
    "    if node_id_counter is None:\n",
    "        node_id_counter = [0]  # Initialize counter\n",
    "    \n",
    "    current_id = node_id_counter[0]\n",
    "    \n",
    "    # Create a label for the current node based on top words\n",
    "    top_words = sorted(node.word_counts.keys(), key=lambda w: node.word_counts[w], reverse=True)[:3]\n",
    "    label = f\"Level {node.level}\\nDocs: {node.documents}\\nWords: {', '.join(top_words)}\"\n",
    "    label_map[current_id] = label\n",
    "    graph.node(str(current_id), label=label)\n",
    "    \n",
    "    # Add edge from parent to current node\n",
    "    if parent_id is not None:\n",
    "        graph.edge(str(parent_id), str(current_id))\n",
    "    \n",
    "    # Traverse children\n",
    "    for child_topic_id, child_node in node.children.items():\n",
    "        node_id_counter[0] += 1  # Increment counter for the child\n",
    "        child_id = node_id_counter[0]\n",
    "        graph, node_id_counter, label_map = visualize_tree_graphviz(\n",
    "            child_node, vocab, graph, parent_id=current_id, node_id_counter=node_id_counter, label_map=label_map\n",
    "        )\n",
    "    \n",
    "    return graph, node_id_counter, label_map\n",
    "\n",
    "def print_tree_graphviz(root, vocab, filename='ncrp_tree', view=False):\n",
    "    \"\"\"\n",
    "    Generate and render the tree visualization using Graphviz.\n",
    "\n",
    "    Args:\n",
    "        root (Node): The root node of the tree.\n",
    "        vocab (list): List of vocabulary words.\n",
    "        filename (str, optional): Filename for the output. Defaults to 'ncrp_tree'.\n",
    "        view (bool, optional): Whether to automatically open the visualization. Defaults to False.\n",
    "    \"\"\"\n",
    "    graph, _, _ = visualize_tree_graphviz(root, vocab)\n",
    "    graph.render(filename, view=view, format='png')\n",
    "    print(f\"Tree visualization saved as {filename}.png\")\n",
    "    \n",
    "def visualize_document_path(tree, doc_id, vocab, filename='doc_path', view=False, top_n=5):\n",
    "    \"\"\"\n",
    "    Visualize the path taken by a particular document through the hierarchical tree.\n",
    "\n",
    "    Args:\n",
    "        tree (nCRPTree): The hierarchical LDA tree.\n",
    "        doc_id (int): ID of the document whose path we want to visualize.\n",
    "        vocab (list): The vocabulary list.\n",
    "        filename (str, optional): The output filename (without extension) for the graph. Defaults to 'doc_path'.\n",
    "        view (bool, optional): Whether to open the generated file after creation. Defaults to False.\n",
    "        top_n (int, optional): Number of top words to display per node. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        None: Saves a .png visualization of the document's path.\n",
    "    \"\"\"\n",
    "    if doc_id not in tree.paths:\n",
    "        print(f\"Document {doc_id} not found in the tree.\")\n",
    "        return\n",
    "\n",
    "    path_nodes = tree.paths[doc_id]\n",
    "\n",
    "    # Create a new Graphviz graph\n",
    "    graph = Digraph(comment=f'Document {doc_id} Path')\n",
    "    graph.attr('node', shape='box', style='filled', color='lightblue')\n",
    "\n",
    "    # Function to get top words\n",
    "    def get_top_words(node, vocab, top_n):\n",
    "        word_counts = list(node.word_counts.items())\n",
    "        word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_words = [w for w, count in word_counts[:top_n]]\n",
    "        return top_words\n",
    "\n",
    "    # Add nodes for each node in the path\n",
    "    node_ids = []\n",
    "    for i, node in enumerate(path_nodes):\n",
    "        tw = get_top_words(node, vocab, top_n)\n",
    "        label = f\"Level {node.level}\\nDocs: {node.documents}\\nTop words: {', '.join(tw)}\"\n",
    "        node_id = f\"{doc_id}_{i}\"\n",
    "        graph.node(node_id, label=label)\n",
    "        node_ids.append(node_id)\n",
    "\n",
    "    # Add edges to represent the path\n",
    "    for i in range(len(node_ids)-1):\n",
    "        graph.edge(node_ids[i], node_ids[i+1])\n",
    "\n",
    "    # Render the graph\n",
    "    graph.render(filename, view=view, format='png')\n",
    "    print(f\"Document {doc_id} path visualization saved as {filename}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 completed.\n",
      "Burn-in period of 5 iterations completed.\n",
      "Iteration 10 completed.\n",
      "Iteration 15 completed.\n",
      "Iteration 20 completed.\n",
      "Gibbs sampling completed.\n",
      "=== Tree Structure After Gibbs Sampling ===\n",
      "Level 0: docs=4, total_words=0\n",
      "  Top words: []\n",
      "  Level 1: docs=2, total_words=0\n",
      "    Top words: []\n",
      "    Level 2: docs=1, total_words=3\n",
      "      Top words: ['apple', 'banana']\n",
      "    Level 2: docs=1, total_words=3\n",
      "      Top words: ['grape', 'apple']\n",
      "  Level 1: docs=2, total_words=0\n",
      "    Top words: []\n",
      "    Level 2: docs=1, total_words=3\n",
      "      Top words: ['banana', 'apple']\n",
      "    Level 2: docs=1, total_words=3\n",
      "      Top words: ['grape', 'banana', 'apple']\n",
      "\n",
      "=== Document Assignments ===\n",
      "Document 0:\n",
      "  apple -> level 2\n",
      "  banana -> level 2\n",
      "  apple -> level 2\n",
      "Document 1:\n",
      "  banana -> level 2\n",
      "  banana -> level 2\n",
      "  apple -> level 2\n",
      "Document 2:\n",
      "  apple -> level 2\n",
      "  grape -> level 2\n",
      "  grape -> level 2\n",
      "Document 3:\n",
      "  grape -> level 2\n",
      "  banana -> level 2\n",
      "  apple -> level 2\n",
      "Iteration 2 completed.\n",
      "Burn-in period of 2 iterations completed.\n",
      "Iteration 4 completed.\n",
      "Gibbs sampling completed.\n",
      "\n",
      "=== Single Document, Single Level Tree ===\n",
      "Level 0: docs=1, total_words=3\n",
      "  Top words: ['apple', 'banana']\n",
      "Document 0:\n",
      "  apple -> level 0\n",
      "  banana -> level 0\n",
      "  apple -> level 0\n",
      "Burn-in period of 2 iterations completed.\n",
      "Iteration 5 completed.\n",
      "Iteration 10 completed.\n",
      "Gibbs sampling completed.\n",
      "\n",
      "=== Small Corpus, Two Levels ===\n",
      "Level 0: docs=2, total_words=0\n",
      "  Top words: []\n",
      "  Level 1: docs=1, total_words=3\n",
      "    Top words: ['cat', 'dog']\n",
      "  Level 1: docs=1, total_words=3\n",
      "    Top words: ['dog', 'cat']\n",
      "Document 0:\n",
      "  cat -> level 1\n",
      "  cat -> level 1\n",
      "  dog -> level 1\n",
      "Document 1:\n",
      "  dog -> level 1\n",
      "  dog -> level 1\n",
      "  cat -> level 1\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: Small Synthetic Corpus\n",
    "corpus = [\n",
    "    [\"apple\", \"banana\", \"apple\"],\n",
    "    [\"banana\", \"banana\", \"apple\"],\n",
    "    [\"apple\", \"grape\", \"grape\"],\n",
    "    [\"grape\", \"banana\", \"apple\"]\n",
    "]\n",
    "vocab = sorted(set(word for doc in corpus for word in doc))\n",
    "\n",
    "tree = nCRPTree(gamma=1.0, eta=0.1, num_levels=3, vocab=vocab, m=0.5, pi=1.0)\n",
    "tree.gibbs_sampling(corpus, num_iterations=20, burn_in=5, thinning=5)\n",
    "\n",
    "print(\"=== Tree Structure After Gibbs Sampling ===\")\n",
    "print_tree(tree.root, vocab)\n",
    "\n",
    "print(\"\\n=== Document Assignments ===\")\n",
    "for doc_id in range(len(corpus)):\n",
    "    print_document_assignments(tree, doc_id)\n",
    "\n",
    "# Test Case 2: Single Document, Single Level\n",
    "corpus_single = [[\"apple\",\"banana\",\"apple\"]]\n",
    "vocab_single = sorted(set(word for doc in corpus_single for word in doc))\n",
    "tree_single = nCRPTree(gamma=1.0, eta=0.1, num_levels=1, vocab=vocab_single)\n",
    "tree_single.gibbs_sampling(corpus_single, num_iterations=5, burn_in=2, thinning=2)\n",
    "print(\"\\n=== Single Document, Single Level Tree ===\")\n",
    "print_tree(tree_single.root, vocab_single)\n",
    "print_document_assignments(tree_single, 0)\n",
    "\n",
    "# Test Case 3: Check with Minimal Depth and Multiple Docs\n",
    "corpus_small = [\n",
    "    [\"cat\", \"cat\", \"dog\"],\n",
    "    [\"dog\", \"dog\", \"cat\"]\n",
    "]\n",
    "vocab_small = sorted(set(word for doc in corpus_small for word in doc))\n",
    "tree_small = nCRPTree(gamma=1.0, eta=0.1, num_levels=2, vocab=vocab_small)\n",
    "tree_small.gibbs_sampling(corpus_small, num_iterations=10, burn_in=2, thinning=5)\n",
    "print(\"\\n=== Small Corpus, Two Levels ===\")\n",
    "print_tree(tree_small.root, vocab_small)\n",
    "for doc_id in range(len(corpus_small)):\n",
    "    print_document_assignments(tree_small, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tree Structure After Gibbs Sampling ===\n",
      "Tree visualization saved as tree_test_case_1.png\n"
     ]
    }
   ],
   "source": [
    "# Visualisation\n",
    "print(\"=== Tree Structure After Gibbs Sampling ===\")\n",
    "print_tree_graphviz(tree.root, vocab, filename='tree_test_case_1', view=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 path visualization saved as doc0_path.png\n"
     ]
    }
   ],
   "source": [
    "visualize_document_path(tree, doc_id=0, vocab=tree.vocab, filename='doc0_path', view=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA3288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
