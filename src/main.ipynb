{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing nCRP tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    total_nodes = 0\n",
    "    last_node_id = 0\n",
    "\n",
    "    def __init__(self, parent=None, level=0):\n",
    "        self.node_id = Node.last_node_id\n",
    "        Node.last_node_id += 1\n",
    "\n",
    "        self.children = {}         # Dictionary to store child nodes\n",
    "        self.documents = 0         # Number of documents passing through this node\n",
    "        self.word_counts = {}      # Word counts at this node\n",
    "        self.total_words = 0       # Total number of words at this node\n",
    "        self.parent = parent       # Parent node\n",
    "        self.level = level         # Level in the tree\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if this node is a leaf node (no children).\"\"\"\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def add_child(self, topic_id):\n",
    "        \"\"\"Adds a child node with a given topic ID.\"\"\"\n",
    "        child_node = Node(parent=self, level=self.level + 1)\n",
    "        self.children[topic_id] = child_node\n",
    "        Node.total_nodes += 1\n",
    "        return child_node\n",
    "\n",
    "    def remove_child(self, topic_id):\n",
    "        \"\"\"Removes a child node with a given topic ID.\"\"\"\n",
    "        if topic_id in self.children:\n",
    "            del self.children[topic_id]\n",
    "            Node.total_nodes -= 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Sampling in nCRPTree\n",
    "\n",
    "Path sampling in the `nCRPTree` involves a sequence of steps to determine the most probable hierarchical path (sequence of nodes) for a given document. The process combines prior probabilities (based on the nested Chinese Restaurant Process, nCRP) and likelihood values (based on the words in the document) to sample a path probabilistically.\n",
    "\n",
    "## Overview of Path Sampling\n",
    "The main function responsible for path sampling is `sample_path`. It orchestrates the process by:\n",
    "1. Removing the document from its current path (if already assigned).\n",
    "2. Collecting word counts for each level of the document.\n",
    "3. Computing the prior probabilities of paths using the nCRP.\n",
    "4. Computing the likelihood of the document for each path.\n",
    "5. Combining the prior and likelihood to compute posterior probabilities over all paths.\n",
    "6. Sampling a new path based on the posterior probabilities.\n",
    "7. Reassigning the document to the sampled path and updating the tree structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Explanation of Each Step\n",
    "\n",
    "### 1. **Remove Document from Current Path**\n",
    "The document is removed from its current path in the tree using the `remove_document` function. This involves:\n",
    "- Traversing the path assigned to the document and decrementing the `documents` count for each node.\n",
    "- Removing nodes if they become empty (i.e., have zero documents).\n",
    "\n",
    "This ensures the document does not bias the path probabilities during re-sampling.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Collect Word Counts for Each Level**\n",
    "The function collects word counts for the document at each level using the `document_words` and `document_levels` inputs. The result is a dictionary, `level_word_counts`, where:\n",
    "- Keys represent levels in the tree.\n",
    "- Values are dictionaries of word frequencies at each level.\n",
    "\n",
    "This is necessary for computing the likelihood of the document given each path.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Compute nCRP Prior**\n",
    "The nested Chinese Restaurant Process (nCRP) prior is computed using the `compute_ncrp_prior` function. This calculates the log-probability of each path based on the hierarchical structure of the tree:\n",
    "$$\n",
    "p(\\text{path}) = \\prod_{\\text{nodes in path}} \\frac{n_i}{\\gamma + n_{\\text{parent}}}\n",
    "$$\n",
    "where:\n",
    "- $n_i$ is the number of documents at the node.\n",
    "- $\\gamma$ is the concentration parameter.\n",
    "- $n_{\\text{parent}}$ is the total number of documents at the parent node.\n",
    "\n",
    "Logarithms are used for numerical stability:\n",
    "$$\n",
    "\\log p(\\text{path}) = \\sum_{\\text{nodes in path}} \\left[ \\log(n_i) - \\log(\\gamma + n_{\\text{parent}}) \\right]\n",
    "$$\n",
    "The prior for creating a new path at a node is:\n",
    "$$\n",
    "\\log p(\\text{new path}) = \\log(\\gamma) - \\log(\\gamma + n_{\\text{parent}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Compute Document Likelihood**\n",
    "The likelihood of the document given a path is computed using the `compute_doc_likelihood` function. The likelihood is based on the Dirichlet-multinomial distribution:\n",
    "$$\n",
    "p(w | z) \\propto \\prod_{i=1}^{\\text{word count}} \\frac{\\eta + \\text{word count in topic} + i - 1}{\\eta_{\\text{sum}} + \\text{total words in topic} + i - 1}\n",
    "$$\n",
    "where:\n",
    "- $\\eta$ is the smoothing parameter for topic-word distributions.\n",
    "- $\\eta_{\\text{sum}}$ is the total smoothing across all words in the vocabulary.\n",
    "\n",
    "The log-likelihood for each node is computed recursively for all levels in the tree.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Compute Posterior Over Paths**\n",
    "The prior and likelihood are combined to compute the posterior probabilities for each path using the `compute_posterior_over_paths` function:\n",
    "$$\n",
    "\\log p(\\text{path} | w) = \\log p(\\text{path}) + \\log p(w | \\text{path})\n",
    "$$\n",
    "To prevent numerical underflow when exponentiating, the log-probabilities are normalized:\n",
    "$$\n",
    "\\text{weights} = \\exp(\\log p(\\text{path} | w) - \\max(\\log p(\\text{path} | w)))\n",
    "$$\n",
    "The probabilities are then normalized to sum to 1:\n",
    "$$\n",
    "\\text{probabilities} = \\frac{\\text{weights}}{\\sum \\text{weights}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Sample a New Path**\n",
    "Using the posterior probabilities, a new path is sampled with the `sample_new_path` function:\n",
    "- A node is chosen probabilistically based on the computed probabilities.\n",
    "- If the sampled node is not a leaf, a new child node is created to extend the path.\n",
    "\n",
    "This step ensures that the tree can dynamically grow to accommodate new topics.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Reassign Document to New Path**\n",
    "The document is reassigned to the sampled path using the `add_document` function. This involves:\n",
    "- Incrementing the `documents` count for each node in the new path.\n",
    "- Updating the word counts at each level of the path to include the words in the document.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Formula Summary\n",
    "The probability of a path is computed as:\n",
    "$$\n",
    "p(\\text{path} | w) \\propto p(\\text{path}) \\cdot p(w | \\text{path})\n",
    "$$\n",
    "where:\n",
    "- $p(\\text{path})$ is the nCRP prior.\n",
    "- $p(w | \\text{path})$ is the document likelihood.\n",
    "\n",
    "The path is sampled using these posterior probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## Functions Involved\n",
    "1. **`remove_document`**: Removes the document from its current path.\n",
    "2. **`compute_ncrp_prior`**: Computes the log-prior for all paths using nCRP.\n",
    "3. **`compute_doc_likelihood`**: Computes the log-likelihood of the document for each path.\n",
    "4. **`compute_posterior_over_paths`**: Combines prior and likelihood to compute posterior probabilities.\n",
    "5. **`sample_new_path`**: Samples a path based on the posterior probabilities.\n",
    "6. **`add_document`**: Reassigns the document to the sampled path and updates the tree.\n",
    "\n",
    "Each of these steps is crucial for ensuring that the path sampling process adheres to the hierarchical probabilistic model described by the nCRP and Dirichlet-multinomial distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nCRPTree:\n",
    "    def __init__(self, alpha, gamma, eta, num_levels, vocab):\n",
    "        \"\"\"\n",
    "        Initialize the nCRP tree.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha: float, smoothing parameter for document-topic distributions.\n",
    "        - gamma: float, concentration parameter for the nested CRP.\n",
    "        - eta: float, smoothing parameter for topic-word distributions.\n",
    "        - num_levels: int, maximum depth of the hierarchical tree.\n",
    "        - vocab: list, vocabulary for the corpus.\n",
    "        \"\"\"\n",
    "        self.root = Node()\n",
    "        self.alpha = alpha  # Smoothing on doc-topic distributions\n",
    "        self.gamma = gamma  # Concentration parameter for nCRP\n",
    "        self.eta = eta      # Smoothing on topic-word distributions\n",
    "        self.eta_sum = eta * len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.num_levels = num_levels\n",
    "        self.paths = {}  # Dictionary to keep track of paths for each document\n",
    "        self.document_leaves = {}  # Mapping from document IDs to their leaf nodes\n",
    "        self.levels = {}  # Mapping from document IDs to word-level assignments\n",
    "\n",
    "    def forget(self):\n",
    "        \"\"\"Reset the tree to its initial state and clear paths.\"\"\"\n",
    "        self.root = Node()\n",
    "        self.paths = {}  # Clear all paths\n",
    "    \n",
    "    def CRP(self, node):\n",
    "        \"\"\"\n",
    "        Basic CRP process.\n",
    "\n",
    "        Returns:\n",
    "        - sampled_topic: Label of the sampled topic (Not the Node)\n",
    "        \"\"\"\n",
    "        total_documents = node.documents  # Including the incoming document\n",
    "        topic_probabilities = {}\n",
    "\n",
    "        # If there are no tables, create a new table with probability 1\n",
    "        if not node.children:\n",
    "            return np.int64(1)  # The new table has a key = 1\n",
    "\n",
    "        else:\n",
    "            # Calculating the probability of joining each of the existing tables (topics)\n",
    "            for topic, child_node in node.children.items():\n",
    "                topic_probabilities[topic] = child_node.documents / (self.alpha + total_documents - 1)\n",
    "\n",
    "            # Probability of creating a new table (topic)\n",
    "            new_table_key = np.max(list(node.children.keys())) + 1\n",
    "            topic_probabilities[new_table_key] = self.alpha / (self.alpha + total_documents - 1)\n",
    "\n",
    "            topics = list(topic_probabilities.keys())\n",
    "            probabilities = list(topic_probabilities.values())\n",
    "\n",
    "            # Since probabilities sum to 1, normalization is not needed\n",
    "            sampled_topic = np.random.choice(topics, p=probabilities)\n",
    "\n",
    "            return sampled_topic\n",
    "\n",
    "    def initialize_new_path(self, max_depth, document_id):\n",
    "        \"\"\"\n",
    "        Sample a path through the tree using the nCRP during initialization.\n",
    "\n",
    "        Parameters:\n",
    "        - max_depth: Maximum depth of the tree (i.e. L-level tree).\n",
    "        - document_id: ID of the document being sampled.\n",
    "\n",
    "        Returns:\n",
    "        - path: A list of topics (node labels) representing the path through the tree.\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        current_node.documents += 1\n",
    "        path = []  # Track the path as a list of node labels (not pointers)\n",
    "\n",
    "        for level in range(1, max_depth):\n",
    "            # Use the CRP function to sample a topic\n",
    "            sampled_topic = self.CRP(current_node)\n",
    "            path.append(sampled_topic)\n",
    "\n",
    "            # Create new table if needed\n",
    "            if sampled_topic not in current_node.children:\n",
    "                current_node.add_child(sampled_topic)\n",
    "\n",
    "            current_node = current_node.children[sampled_topic]\n",
    "            current_node.documents += 1\n",
    "\n",
    "        self.paths[document_id] = path\n",
    "        return path\n",
    "    \n",
    "\n",
    "    def initialise_tree(self, corpus, max_depth):\n",
    "        \"\"\"\n",
    "        Initialise the tree by sampling a path for each document using the nCRP\n",
    "        and assigning word counts to nodes along their respective paths.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus: List of documents (each document is a list of words)\n",
    "        - max_depth: Maximum depth of the tree\n",
    "        \"\"\"\n",
    "        for doc_id, doc_words in enumerate(corpus):\n",
    "            path = self.initialize_new_path(max_depth, doc_id)\n",
    "\n",
    "            # Traverse the path and assign words to nodes along the path\n",
    "            current_node = self.root\n",
    "            current_node.documents += 1  # Increment document count at the root\n",
    "            for level, topic_id in enumerate(path):\n",
    "                if topic_id not in current_node.children:\n",
    "                    current_node.add_child(topic_id)\n",
    "                current_node = current_node.children[topic_id]\n",
    "\n",
    "                # Assign word counts to this node\n",
    "                current_node.documents += 1\n",
    "                for word in doc_words:\n",
    "                    current_node.word_counts[word] = current_node.word_counts.get(word, 0) + 1\n",
    "                    current_node.total_words += 1\n",
    "\n",
    "    def _set_pointers(self, path):\n",
    "        \"\"\"\n",
    "        Traverse the path and set pointers to the nodes along the path.\n",
    "\n",
    "        Parameters:\n",
    "        - path: A list of node labels (topics) representing the path through the tree\n",
    "\n",
    "        Returns:\n",
    "        - node_pointers: A list of pointers to the actual nodes along the path\n",
    "        \"\"\"\n",
    "        node_pointers = []\n",
    "        current_node = self.root\n",
    "\n",
    "        for topic in path:\n",
    "            current_node = current_node.children[topic]\n",
    "            node_pointers.append(current_node)\n",
    "\n",
    "        return node_pointers\n",
    "    \n",
    "    def add_document(self, document_id, sampled_node, level_word_counts):\n",
    "        path = []\n",
    "        node = sampled_node\n",
    "        while node is not None:\n",
    "            path.insert(0, node)\n",
    "            node.documents += 1\n",
    "            node = node.parent\n",
    "        self.paths[document_id] = path\n",
    "        # Update word counts along the path\n",
    "        for level, node in enumerate(path):\n",
    "            word_counts = level_word_counts.get(level, {})\n",
    "            for word, count in word_counts.items():\n",
    "                node.word_counts[word] = node.word_counts.get(word, 0) + count\n",
    "                node.total_words += count\n",
    "    \n",
    "    def remove_document(self, document_id):\n",
    "        \"\"\"\n",
    "        Remove a document from its current path in the tree and update statistics.\n",
    "\n",
    "        Parameters:\n",
    "        - document_id: int, ID of the document to be removed.\n",
    "        \"\"\"\n",
    "        if document_id not in self.paths:\n",
    "            return  # Document is not in the tree, no action needed\n",
    "\n",
    "        path = self.paths[document_id]  # Get the path for this document\n",
    "        current_node = self.root\n",
    "\n",
    "        # Iterate over the path and decrement counts\n",
    "        for topic in path:\n",
    "            # Move to the child node\n",
    "            child_node = current_node.children.get(topic)\n",
    "            if child_node is None:\n",
    "                continue\n",
    "\n",
    "            # Decrement the document count\n",
    "            child_node.documents -= 1\n",
    "\n",
    "            # Remove the node if it's empty\n",
    "            if child_node.documents == 0:\n",
    "                del current_node.children[topic]\n",
    "                Node.total_nodes -= 1\n",
    "            else:\n",
    "                # Adjust word counts for this node\n",
    "                if document_id in self.levels:\n",
    "                    word_counts_at_level = self.levels[document_id].get(child_node.level, {})\n",
    "                    for word, count in word_counts_at_level.items():\n",
    "                        child_node.word_counts[word] -= count\n",
    "                        if child_node.word_counts[word] == 0:\n",
    "                            del child_node.word_counts[word]\n",
    "                        child_node.total_words -= count\n",
    "\n",
    "            # Move to the next node\n",
    "            current_node = child_node\n",
    "\n",
    "        # Remove the document from paths and levels\n",
    "        del self.paths[document_id]\n",
    "        if document_id in self.levels:\n",
    "            del self.levels[document_id]\n",
    "    \n",
    "    def compute_ncrp_prior(self, node, weight, node_weights):\n",
    "        \"\"\"\n",
    "        Compute the nested CRP prior recursively for all paths from the given node.\n",
    "\n",
    "        Parameters:\n",
    "        - node: Node object, the starting node for the computation.\n",
    "        - weight: float, cumulative log probability of the path up to the current node.\n",
    "        - node_weights: dict, stores the computed log probabilities for all nodes.\n",
    "        \"\"\"\n",
    "        total_customers = node.documents\n",
    "        for topic_id, child_node in node.children.items():\n",
    "            child_weight = weight + np.log(child_node.documents / (self.gamma + total_customers))\n",
    "            self.compute_ncrp_prior(child_node, child_weight, node_weights)\n",
    "            \n",
    "        # Weight for creating a new path from this node\n",
    "        new_path_weight = weight + np.log(self.gamma / (self.gamma + total_customers))\n",
    "        node_weights[node] = new_path_weight\n",
    "    \n",
    "    def compute_doc_likelihood(self, node, level_word_counts, weight, node_weights, level=0):\n",
    "        \"\"\"\n",
    "        Compute the document likelihood for words at each level along the path.\n",
    "\n",
    "        Parameters:\n",
    "        - node: Node object, the starting node for likelihood computation.\n",
    "        - level_word_counts: dict, mapping levels to word counts for the document.\n",
    "        - weight: float, cumulative log likelihood up to the current node.\n",
    "        - node_weights: dict, stores the computed likelihood values for nodes.\n",
    "        - level: int, current level of the tree (default is 0).\n",
    "        \"\"\"\n",
    "        node_weight = 0.0\n",
    "        word_counts = level_word_counts.get(level, {})\n",
    "        total_words = node.total_words\n",
    "        for word, count in word_counts.items():\n",
    "            word_count_at_node = node.word_counts.get(word, 0)\n",
    "            for i in range(count):\n",
    "                node_weight += np.log((self.eta + word_count_at_node + i) /\n",
    "                                    (self.eta_sum + total_words + i))\n",
    "        total_weight = weight + node_weight\n",
    "        node_weights[node] += total_weight\n",
    "        # Recurse into children\n",
    "        for child_node in node.children.values():\n",
    "            self.compute_doc_likelihood(child_node, level_word_counts, total_weight, node_weights, level + 1)\n",
    "        \n",
    "    def compute_posterior_over_paths(self, node_weights):\n",
    "        \"\"\"\n",
    "        Compute the posterior probabilities over paths based on prior and likelihood.\n",
    "\n",
    "        Parameters:\n",
    "        - node_weights: dict, combined prior and likelihood values for nodes.\n",
    "\n",
    "        Returns:\n",
    "        - nodes: list, the Node objects for which the posterior is computed.\n",
    "        - probabilities: numpy array, normalized probabilities for the nodes.\n",
    "        \"\"\"\n",
    "        nodes = list(node_weights.keys())\n",
    "        weights = np.array(list(node_weights.values()))\n",
    "        max_weight = np.max(weights)\n",
    "        weights = np.exp(weights - max_weight)  # For numerical stability\n",
    "        probabilities = weights / np.sum(weights)\n",
    "        return nodes, probabilities\n",
    "    \n",
    "    def sample_new_path(self, nodes, probabilities):\n",
    "        \"\"\"\n",
    "        Sample a new path based on posterior probabilities.\n",
    "\n",
    "        Parameters:\n",
    "        - nodes: list, Node objects to sample from.\n",
    "        - probabilities: numpy array, probabilities corresponding to the nodes.\n",
    "\n",
    "        Returns:\n",
    "        - sampled_node: Node object, the sampled node.\n",
    "        \"\"\"\n",
    "        sampled_node = np.random.choice(nodes, p=probabilities)\n",
    "        # If the sampled node is not a leaf, we need to create a new leaf from it\n",
    "        if not sampled_node.is_leaf():\n",
    "            new_topic_id = max(sampled_node.children.keys(), default=0) + 1\n",
    "            sampled_node = sampled_node.add_child(new_topic_id)\n",
    "        return sampled_node\n",
    "                \n",
    "    def sample_path(self, document_id, document_words, document_levels):\n",
    "        # Remove the document from its current path\n",
    "        if document_id in self.paths:\n",
    "            self.remove_document(document_id)\n",
    "        # Collect word counts per level\n",
    "        level_word_counts = {}\n",
    "        for word, level in zip(document_words, document_levels):\n",
    "            level_word_counts.setdefault(level, {})\n",
    "            level_word_counts[level][word] = level_word_counts[level].get(word, 0) + 1\n",
    "        # Compute the nCRP prior\n",
    "        node_weights = defaultdict(float)\n",
    "        self.compute_ncrp_prior(self.root, 0.0, node_weights)\n",
    "        # Compute the document likelihood for each node\n",
    "        self.compute_doc_likelihood(self.root, level_word_counts, 0.0, node_weights)\n",
    "        # Compute posterior over paths\n",
    "        nodes, probabilities = self.compute_posterior_over_paths(node_weights)\n",
    "        # Sample a new path\n",
    "        sampled_node = self.sample_new_path(nodes, probabilities)\n",
    "        # Add the document back to the tree\n",
    "        self.add_document(document_id, sampled_node, level_word_counts)\n",
    "        # Update the document's leaf node\n",
    "        self.document_leaves[document_id] = sampled_node\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tree...\n",
      "\n",
      "Tree Structure After Initialization:\n",
      "Root Node: Documents=6\n",
      "  Topic 1: Level=1, Documents=4, Total Words=4\n",
      "    Sub-Topic 1: Level=2, Documents=4, Total Words=4\n",
      "  Topic 2: Level=1, Documents=2, Total Words=2\n",
      "    Sub-Topic 1: Level=2, Documents=2, Total Words=2\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary and corpus\n",
    "vocab = [\"a\", \"b\", \"c\"]\n",
    "corpus = [[0, 1], [1, 2], [0, 2]]  # Toy corpus with 3 documents\n",
    "tree = nCRPTree(alpha=1.0, gamma=1.0, eta=0.1, num_levels=3, vocab=vocab)\n",
    "\n",
    "print(\"Initializing tree...\")\n",
    "tree.initialise_tree(corpus, max_depth=3)\n",
    "\n",
    "print(\"\\nTree Structure After Initialization:\")\n",
    "print(f\"Root Node: Documents={tree.root.documents}\")\n",
    "for topic_id, child_node in tree.root.children.items():\n",
    "    print(f\"  Topic {topic_id}: Level={child_node.level}, Documents={child_node.documents}, Total Words={child_node.total_words}\")\n",
    "    for sub_topic_id, sub_child_node in child_node.children.items():\n",
    "        print(f\"    Sub-Topic {sub_topic_id}: Level={sub_child_node.level}, Documents={sub_child_node.documents}, Total Words={sub_child_node.total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removing document 1...\n",
      "\n",
      "Tree Structure After Document Removal:\n",
      "Root Node: Documents=6\n",
      "  Topic 1: Level=1, Documents=4, Total Words=4\n",
      "    Sub-Topic 1: Level=2, Documents=4, Total Words=4\n",
      "  Topic 2: Level=1, Documents=1, Total Words=2\n",
      "    Sub-Topic 1: Level=2, Documents=1, Total Words=2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRemoving document 1...\")\n",
    "tree.remove_document(document_id=1)\n",
    "\n",
    "print(\"\\nTree Structure After Document Removal:\")\n",
    "print(f\"Root Node: Documents={tree.root.documents}\")\n",
    "for topic_id, child_node in tree.root.children.items():\n",
    "    print(f\"  Topic {topic_id}: Level={child_node.level}, Documents={child_node.documents}, Total Words={child_node.total_words}\")\n",
    "    for sub_topic_id, sub_child_node in child_node.children.items():\n",
    "        print(f\"    Sub-Topic {sub_topic_id}: Level={sub_child_node.level}, Documents={sub_child_node.documents}, Total Words={sub_child_node.total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling a path for a new document...\n",
      "\n",
      "Tree Structure After Sampling New Path:\n",
      "Root Node: Documents=7\n",
      "  Topic 1: Level=1, Documents=4, Total Words=4\n",
      "    Sub-Topic 1: Level=2, Documents=4, Total Words=4\n",
      "  Topic 2: Level=1, Documents=1, Total Words=2\n",
      "    Sub-Topic 1: Level=2, Documents=1, Total Words=2\n",
      "  Topic 3: Level=1, Documents=1, Total Words=1\n"
     ]
    }
   ],
   "source": [
    "new_document = [0, 1, 2]\n",
    "print(\"\\nSampling a path for a new document...\")\n",
    "tree.sample_path(document_id=3, document_words=new_document, document_levels=[0, 1, 2])\n",
    "\n",
    "print(\"\\nTree Structure After Sampling New Path:\")\n",
    "print(f\"Root Node: Documents={tree.root.documents}\")\n",
    "for topic_id, child_node in tree.root.children.items():\n",
    "    print(f\"  Topic {topic_id}: Level={child_node.level}, Documents={child_node.documents}, Total Words={child_node.total_words}\")\n",
    "    for sub_topic_id, sub_child_node in child_node.children.items():\n",
    "        print(f\"    Sub-Topic {sub_topic_id}: Level={sub_child_node.level}, Documents={sub_child_node.documents}, Total Words={sub_child_node.total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing nCRP prior...\n",
      "\n",
      "Computed nCRP Priors:\n",
      "Node 2 (Level 2): Prior Weight=-2.525728644308255\n",
      "Node 1 (Level 1): Prior Weight=-2.3025850929940455\n",
      "Node 4 (Level 2): Prior Weight=-3.4657359027997265\n",
      "Node 3 (Level 1): Prior Weight=-2.772588722239781\n",
      "Node 5 (Level 1): Prior Weight=-2.772588722239781\n",
      "Node 0 (Level 0): Prior Weight=-2.0794415416798357\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing nCRP prior...\")\n",
    "node_weights = defaultdict(float)\n",
    "tree.compute_ncrp_prior(tree.root, 0.0, node_weights)\n",
    "\n",
    "print(\"\\nComputed nCRP Priors:\")\n",
    "for node, weight in node_weights.items():\n",
    "    print(f\"Node {node.node_id} (Level {node.level}): Prior Weight={weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing document likelihood...\n",
      "\n",
      "Computed Document Likelihoods:\n",
      "Node 0 (Level 0): Likelihood Weight=-0.25802586286889284\n",
      "Node 1 (Level 1): Likelihood Weight=-1.621330705764085\n",
      "Node 2 (Level 2): Likelihood Weight=-2.984635548659277\n",
      "Node 3 (Level 1): Likelihood Weight=-0.9956248059996718\n",
      "Node 4 (Level 2): Likelihood Weight=-1.7332237491304507\n",
      "Node 5 (Level 1): Likelihood Weight=-0.425079947532059\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing document likelihood...\")\n",
    "level_word_counts = {0: {0: 2}, 1: {1: 1}, 2: {2: 1}}  # Example word counts for a document\n",
    "node_weights = defaultdict(float)\n",
    "tree.compute_doc_likelihood(tree.root, level_word_counts, 0.0, node_weights)\n",
    "\n",
    "print(\"\\nComputed Document Likelihoods:\")\n",
    "for node, weight in node_weights.items():\n",
    "    print(f\"Node {node.node_id} (Level {node.level}): Likelihood Weight={weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing posterior over paths...\n",
      "\n",
      "Posterior Probabilities:\n",
      "Node 0 (Level 0): Probability=0.3478983400128264\n",
      "Node 1 (Level 1): Probability=0.088997249770723\n",
      "Node 2 (Level 2): Probability=0.02276673831344076\n",
      "Node 3 (Level 1): Probability=0.16638616261483002\n",
      "Node 4 (Level 2): Probability=0.07957599081578828\n",
      "Node 5 (Level 1): Probability=0.2943755184723915\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing posterior over paths...\")\n",
    "nodes, probabilities = tree.compute_posterior_over_paths(node_weights)\n",
    "\n",
    "print(\"\\nPosterior Probabilities:\")\n",
    "for node, prob in zip(nodes, probabilities):\n",
    "    print(f\"Node {node.node_id} (Level {node.level}): Probability={prob}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA3288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
