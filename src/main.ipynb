{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import log, exp\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, parent=None, level=0):\n",
    "        self.children = {}         # Dictionary to store child nodes {topic_id: Node}\n",
    "        self.documents = 0         # Number of documents passing through this node\n",
    "        self.word_counts = defaultdict(int)  # Word counts at this node\n",
    "        self.total_words = 0       # Total number of words at this node\n",
    "        self.parent = parent       # Parent node\n",
    "        self.level = level         # Level in the tree\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if this node is a leaf node (no children).\"\"\"\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def add_child(self, topic_id):\n",
    "        \"\"\"Adds a child node with a given topic ID.\"\"\"\n",
    "        child_node = Node(parent=self, level=self.level + 1)\n",
    "        self.children[topic_id] = child_node\n",
    "        return child_node\n",
    "\n",
    "    def remove_child(self, topic_id):\n",
    "        \"\"\"Removes a child node with a given topic ID.\"\"\"\n",
    "        if topic_id in self.children:\n",
    "            del self.children[topic_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nCRPTree:\n",
    "    def __init__(self, alpha, gamma, eta, num_levels, vocab):\n",
    "        \"\"\"\n",
    "        Initialize the nCRP tree.\n",
    "\n",
    "        Parameters:\n",
    "        - alpha: float, smoothing parameter for document-topic distributions.\n",
    "        - gamma: float, concentration parameter for the nested CRP.\n",
    "        - eta: float, smoothing parameter for topic-word distributions.\n",
    "        - num_levels: int, maximum depth of the hierarchical tree.\n",
    "        - vocab: list, vocabulary for the corpus.\n",
    "        \"\"\"\n",
    "        self.root = Node()\n",
    "        self.alpha = alpha          # Smoothing on doc-topic distributions\n",
    "        self.gamma = gamma          # Concentration parameter for nCRP\n",
    "        self.eta = eta              # Smoothing on topic-word distributions\n",
    "        self.eta_sum = eta * len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.num_levels = num_levels\n",
    "        self.paths = {}             # Dictionary to keep track of paths for each document\n",
    "        self.levels = {}            # Mapping from document IDs to word-level assignments\n",
    "        self.document_words = {}    # Mapping from document_id to list of words in the document\n",
    "\n",
    "    def CRP(self, node):\n",
    "        \"\"\"\n",
    "        Basic CRP process.\n",
    "\n",
    "        Returns:\n",
    "        - sampled_topic: Label of the sampled topic (Not the Node)\n",
    "        \"\"\"\n",
    "        total_words = node.total_words  # Use total words at the current node\n",
    "        topic_probabilities = {}\n",
    "\n",
    "        # If there are no children, create a new topic\n",
    "        if not node.children:\n",
    "            return 1  # Create a new topic with key = 1\n",
    "\n",
    "        # Calculating probabilities for existing topics\n",
    "        for topic, child_node in node.children.items():\n",
    "            topic_probabilities[topic] = child_node.total_words / (self.alpha + total_words)\n",
    "\n",
    "        # Probability of creating a new topic\n",
    "        new_topic_key = max(node.children.keys(), default=0) + 1\n",
    "        topic_probabilities[new_topic_key] = self.alpha / (self.alpha + total_words)\n",
    "\n",
    "        # Normalize probabilities\n",
    "        topics = list(topic_probabilities.keys())\n",
    "        probabilities = list(topic_probabilities.values())\n",
    "        total_prob = sum(probabilities)\n",
    "        probabilities = [p / total_prob for p in probabilities]  # Normalize\n",
    "\n",
    "        # Ensure sum of probabilities is 1 (for debugging purposes)\n",
    "        assert abs(sum(probabilities) - 1.0) < 1e-6, \"Probabilities do not sum to 1\"\n",
    "        return np.random.choice(topics, p=probabilities)\n",
    "\n",
    "    def initialize_new_path(self, max_depth, document_id):\n",
    "        \"\"\"\n",
    "        Sample a path through the tree using the nCRP during initialization.\n",
    "\n",
    "        Parameters:\n",
    "        - max_depth: Maximum depth of the tree (i.e., L-level tree).\n",
    "        - document_id: ID of the document being sampled.\n",
    "\n",
    "        Returns:\n",
    "        - path_nodes: A list of Node objects representing the path through the tree.\n",
    "        \"\"\"\n",
    "        current_node = self.root\n",
    "        current_node.documents += 1\n",
    "        path_nodes = [current_node]  # Start with the root node\n",
    "\n",
    "        for level in range(1, max_depth):\n",
    "            # Use the CRP function to sample a topic\n",
    "            sampled_topic = self.CRP(current_node)\n",
    "\n",
    "            # Create new child if needed\n",
    "            if sampled_topic not in current_node.children:\n",
    "                child_node = current_node.add_child(sampled_topic)\n",
    "            else:\n",
    "                child_node = current_node.children[sampled_topic]\n",
    "\n",
    "            child_node.documents += 1\n",
    "            path_nodes.append(child_node)\n",
    "            current_node = child_node\n",
    "\n",
    "        self.paths[document_id] = path_nodes  # Store path as node pointers\n",
    "        return path_nodes\n",
    "\n",
    "    def initialise_tree(self, corpus, max_depth):\n",
    "        \"\"\"\n",
    "        Initialize the tree by sampling a path for each document using the nCRP\n",
    "        and assigning word counts to nodes along their respective paths.\n",
    "\n",
    "        Parameters:\n",
    "        - corpus: List of documents (each document is a list of word indices)\n",
    "        - max_depth: Maximum depth of the tree\n",
    "        \"\"\"\n",
    "        for doc_id, doc_words in enumerate(corpus):\n",
    "            # Store document words for easy access later\n",
    "            self.document_words[doc_id] = doc_words\n",
    "\n",
    "            # Sample a path for this document\n",
    "            path_nodes = self.initialize_new_path(max_depth, doc_id)\n",
    "\n",
    "            # Randomly assign levels to words and update word counts\n",
    "            doc_levels = []\n",
    "            num_levels = len(path_nodes)\n",
    "            for word in doc_words:\n",
    "                level = np.random.randint(0, num_levels)\n",
    "                doc_levels.append(level)\n",
    "                node = path_nodes[level]\n",
    "                node.word_counts[word] += 1\n",
    "                node.total_words += 1\n",
    "\n",
    "            # Store levels for this document\n",
    "            self.levels[doc_id] = doc_levels\n",
    "\n",
    "    def add_document(self, document_id, path_nodes, level_word_counts):\n",
    "        \"\"\"\n",
    "        Add a document to the tree by updating path and word counts.\n",
    "\n",
    "        Parameters:\n",
    "        - document_id: int, ID of the document\n",
    "        - path_nodes: list of Node objects representing the path\n",
    "        - level_word_counts: dict mapping levels to word counts\n",
    "        \"\"\"\n",
    "        self.paths[document_id] = path_nodes\n",
    "        # Increment document counts along the path\n",
    "        for node in path_nodes:\n",
    "            node.documents += 1\n",
    "        # Update word counts along the path\n",
    "        for level, word_counts in level_word_counts.items():\n",
    "            node = path_nodes[level]\n",
    "            for word, count in word_counts.items():\n",
    "                node.word_counts[word] += count\n",
    "                node.total_words += count\n",
    "\n",
    "    def remove_document(self, document_id):\n",
    "        \"\"\"\n",
    "        Remove a document from its current path in the tree and update attributes.\n",
    "\n",
    "        Parameters:\n",
    "        - document_id: int, ID of the document to be removed.\n",
    "        \"\"\"\n",
    "        if document_id not in self.paths:\n",
    "            return  # Document is not in the tree, no action needed\n",
    "\n",
    "        path_nodes = self.paths[document_id]  # List of node pointers\n",
    "        doc_levels = self.levels[document_id]  # List of levels assigned to each word\n",
    "        doc_words = self.document_words[document_id]\n",
    "\n",
    "        # Decrement word counts at the nodes along the path\n",
    "        for word, level in zip(doc_words, doc_levels):\n",
    "            node = path_nodes[level]\n",
    "            node.word_counts[word] -= 1\n",
    "            if node.word_counts[word] == 0:\n",
    "                del node.word_counts[word]\n",
    "            node.total_words -= 1\n",
    "\n",
    "        # Decrement document counts along the path\n",
    "        for node in path_nodes:\n",
    "            node.documents -= 1\n",
    "\n",
    "        # Remove nodes that have zero documents and no children\n",
    "        for node in reversed(path_nodes):\n",
    "            if node.documents == 0 and node.is_leaf():\n",
    "                parent = node.parent\n",
    "                if parent:\n",
    "                    # Find the topic_id corresponding to this child\n",
    "                    topic_id_to_remove = None\n",
    "                    for topic_id, child in parent.children.items():\n",
    "                        if child == node:\n",
    "                            topic_id_to_remove = topic_id\n",
    "                            break\n",
    "                    if topic_id_to_remove is not None:\n",
    "                        parent.remove_child(topic_id_to_remove)\n",
    "\n",
    "        # Remove document from paths, levels, and document_words\n",
    "        del self.paths[document_id]\n",
    "        del self.levels[document_id]\n",
    "        del self.document_words[document_id]\n",
    "\n",
    "    def compute_ncrp_prior(self, node, weight, node_weights):\n",
    "        \"\"\"\n",
    "        Compute the nested CRP prior recursively for all paths from the given node.\n",
    "\n",
    "        Parameters:\n",
    "        - node: Node object, the starting node for the computation.\n",
    "        - weight: float, cumulative log probability of the path up to the current node.\n",
    "        - node_weights: dict, stores the computed log probabilities for all nodes.\n",
    "        \"\"\"\n",
    "        total_customers = node.documents\n",
    "        for topic_id, child_node in node.children.items():\n",
    "            child_weight = weight + np.log(child_node.documents / (self.gamma + total_customers))\n",
    "            self.compute_ncrp_prior(child_node, child_weight, node_weights)\n",
    "\n",
    "        # Weight for creating a new path (topic) from this node\n",
    "        new_path_weight = weight + np.log(self.gamma / (self.gamma + total_customers))\n",
    "        node_weights[node] += new_path_weight  # Accumulate weight\n",
    "\n",
    "    def compute_doc_likelihood(self, node, level_word_counts, weight, node_weights, level=0):\n",
    "        \"\"\"\n",
    "        Compute the document likelihood for words at each level along the path.\n",
    "\n",
    "        Parameters:\n",
    "        - node: Node object, the starting node for likelihood computation.\n",
    "        - level_word_counts: dict, mapping levels to word counts for the document.\n",
    "        - weight: float, cumulative log likelihood up to the current node.\n",
    "        - node_weights: dict, stores the computed likelihood values for nodes.\n",
    "        - level: int, current level of the tree (default is 0).\n",
    "        \"\"\"\n",
    "        node_weight = 0.0\n",
    "        word_counts = level_word_counts.get(level, {})\n",
    "        total_words = node.total_words\n",
    "        for word, count in word_counts.items():\n",
    "            word_count_at_node = node.word_counts.get(word, 0)\n",
    "            for _ in range(count):\n",
    "                node_weight += np.log((self.eta + word_count_at_node) / (self.eta_sum + total_words))\n",
    "\n",
    "        total_weight = weight + node_weight\n",
    "        node_weights[node] += total_weight\n",
    "        # Recurse into children\n",
    "        for child_node in node.children.values():\n",
    "            self.compute_doc_likelihood(child_node, level_word_counts, total_weight, node_weights, level + 1)\n",
    "\n",
    "    def compute_posterior_over_paths(self, node_weights):\n",
    "        \"\"\"\n",
    "        Compute the posterior probabilities over paths based on prior and likelihood.\n",
    "\n",
    "        Parameters:\n",
    "        - node_weights: dict, combined prior and likelihood values for nodes.\n",
    "\n",
    "        Returns:\n",
    "        - nodes: list, the Node objects for which the posterior is computed.\n",
    "        - probabilities: numpy array, normalized probabilities for the nodes.\n",
    "        \"\"\"\n",
    "        nodes = list(node_weights.keys())\n",
    "        weights = np.array(list(node_weights.values()))\n",
    "        max_weight = np.max(weights)\n",
    "        weights = np.exp(weights - max_weight)  # For numerical stability\n",
    "        probabilities = weights / np.sum(weights)\n",
    "        return nodes, probabilities\n",
    "\n",
    "    def sample_new_path(self, nodes, probabilities):\n",
    "        \"\"\"\n",
    "        Sample a new path based on posterior probabilities.\n",
    "\n",
    "        Parameters:\n",
    "        - nodes: list, Node objects to sample from.\n",
    "        - probabilities: numpy array, probabilities corresponding to the nodes.\n",
    "\n",
    "        Returns:\n",
    "        - sampled_node: Node object, the sampled node.\n",
    "        \"\"\"\n",
    "        sampled_node = np.random.choice(nodes, p=probabilities)\n",
    "        # If the sampled node is not a leaf, create a new child under it\n",
    "        if not sampled_node.is_leaf():\n",
    "            new_topic_id = max(sampled_node.children.keys(), default=0) + 1\n",
    "            sampled_node = sampled_node.add_child(new_topic_id)\n",
    "        return sampled_node\n",
    "\n",
    "    def sample_path(self, document_id, document_words, document_levels):\n",
    "        \"\"\"\n",
    "        Sample a new path for a document.\n",
    "\n",
    "        Parameters:\n",
    "        - document_id: int, ID of the document\n",
    "        - document_words: list, words in the document\n",
    "        - document_levels: list, levels assigned to each word\n",
    "        \"\"\"\n",
    "        # Remove the document from its current path\n",
    "        if document_id in self.paths:\n",
    "            self.remove_document(document_id)\n",
    "\n",
    "        # Collect word counts per level\n",
    "        level_word_counts = {}\n",
    "        for word, level in zip(document_words, document_levels):\n",
    "            level_word_counts.setdefault(level, {})\n",
    "            level_word_counts[level][word] = level_word_counts[level].get(word, 0) + 1\n",
    "\n",
    "        # Compute the nCRP prior\n",
    "        node_weights = defaultdict(float)\n",
    "        self.compute_ncrp_prior(self.root, 0.0, node_weights)\n",
    "\n",
    "        # Compute the document likelihood for each node\n",
    "        self.compute_doc_likelihood(self.root, level_word_counts, 0.0, node_weights)\n",
    "\n",
    "        # Compute posterior over paths\n",
    "        nodes, probabilities = self.compute_posterior_over_paths(node_weights)\n",
    "\n",
    "        # Sample a new path\n",
    "        sampled_node = self.sample_new_path(nodes, probabilities)\n",
    "\n",
    "        # Build the path from root to the sampled node\n",
    "        path_nodes = []\n",
    "        node = sampled_node\n",
    "        while node is not None:\n",
    "            path_nodes.insert(0, node)\n",
    "            node = node.parent\n",
    "\n",
    "        # Add the document back to the tree\n",
    "        self.add_document(document_id, path_nodes, level_word_counts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tree...\n",
      "\n",
      "Tree Structure After Initialization:\n",
      "Root Node: Documents=3, Total Words=0\n",
      "  Topic 1: Level=1, Documents=3, Total Words=2\n",
      "    Topic 1: Level=2, Documents=1, Total Words=1\n",
      "    Topic 2: Level=2, Documents=2, Total Words=3\n",
      "\n",
      "Total Words in Tree: 6 (Expected: 6)\n",
      "\n",
      "Removing document 1...\n",
      "\n",
      "Tree Structure After Document Removal:\n",
      "Root Node: Documents=2, Total Words=0\n",
      "  Topic 1: Level=1, Documents=2, Total Words=1\n",
      "    Topic 1: Level=2, Documents=1, Total Words=1\n",
      "    Topic 2: Level=2, Documents=1, Total Words=2\n",
      "\n",
      "Total Words in Tree: 4 (Expected: 4)\n",
      "\n",
      "Adding a new document (Document 3) with words [1, 2]:\n",
      "\n",
      "Tree Structure After Adding Document 3:\n",
      "Root Node: Documents=4, Total Words=2\n",
      "  Topic 1: Level=1, Documents=2, Total Words=1\n",
      "    Topic 1: Level=2, Documents=1, Total Words=1\n",
      "    Topic 2: Level=2, Documents=1, Total Words=2\n",
      "  Topic 2: Level=1, Documents=2, Total Words=0\n",
      "    Topic 1: Level=2, Documents=2, Total Words=0\n",
      "\n",
      "Total Words in Tree: 6 (Expected: 6)\n",
      "\n",
      "Attempting to remove a non-existent document (Document 999)...\n",
      "\n",
      "Tree Structure After Attempting to Remove Non-Existent Document:\n",
      "Root Node: Documents=4, Total Words=2\n",
      "  Topic 1: Level=1, Documents=2, Total Words=1\n",
      "    Topic 1: Level=2, Documents=1, Total Words=1\n",
      "    Topic 2: Level=2, Documents=1, Total Words=2\n",
      "  Topic 2: Level=1, Documents=2, Total Words=0\n",
      "    Topic 1: Level=2, Documents=2, Total Words=0\n",
      "\n",
      "Total Words in Tree: 6 (Expected: 6)\n",
      "\n",
      "All test cases passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def print_tree(node, indent=0):\n",
    "    \"\"\"\n",
    "    Helper function to print the tree structure.\n",
    "\n",
    "    Parameters:\n",
    "    - node: Node object, starting node (default is root)\n",
    "    - indent: int, current indentation level\n",
    "    \"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    if node.parent is None:\n",
    "        print(f\"{prefix}Root Node: Documents={node.documents}, Total Words={node.total_words}\")\n",
    "    else:\n",
    "        # Find the topic ID\n",
    "        parent = node.parent\n",
    "        topic_id = None\n",
    "        for tid, child in parent.children.items():\n",
    "            if child == node:\n",
    "                topic_id = tid\n",
    "                break\n",
    "        print(f\"{prefix}Topic {topic_id}: Level={node.level}, Documents={node.documents}, Total Words={node.total_words}\")\n",
    "    for child in node.children.values():\n",
    "        print_tree(child, indent + 1)\n",
    "\n",
    "def sum_total_words(node):\n",
    "    \"\"\"Helper function to sum total_words in the tree.\"\"\"\n",
    "    total = node.total_words\n",
    "    for child in node.children.values():\n",
    "        total += sum_total_words(child)\n",
    "    return total\n",
    "\n",
    "def compute_total_words(node):\n",
    "    \"\"\"Helper function to compute total words from word_counts.\"\"\"\n",
    "    total = sum(node.word_counts.values())\n",
    "    for child in node.children.values():\n",
    "        total += compute_total_words(child)\n",
    "    return total\n",
    "\n",
    "def main():\n",
    "    # Vocabulary and corpus\n",
    "    vocab = [\"a\", \"b\", \"c\"]\n",
    "    corpus = [\n",
    "        [0, 1],    # Document 0\n",
    "        [1, 2],    # Document 1\n",
    "        [0, 2],    # Document 2\n",
    "    ]  # Toy corpus with 3 documents\n",
    "    tree = nCRPTree(alpha=1.0, gamma=1.0, eta=0.1, num_levels=3, vocab=vocab)\n",
    "\n",
    "    print(\"Initializing tree...\")\n",
    "    tree.initialise_tree(corpus, max_depth=3)\n",
    "\n",
    "    print(\"\\nTree Structure After Initialization:\")\n",
    "    print_tree(tree.root)\n",
    "\n",
    "    # Verify total_words consistency\n",
    "    total_words = sum(len(doc) for doc in corpus)\n",
    "    tree_total_words = sum_total_words(tree.root)\n",
    "    print(f\"\\nTotal Words in Tree: {tree_total_words} (Expected: {total_words})\")\n",
    "    assert tree_total_words == total_words, \"Total words mismatch after initialization!\"\n",
    "\n",
    "    print(\"\\nRemoving document 1...\")\n",
    "    tree.remove_document(document_id=1)\n",
    "\n",
    "    print(\"\\nTree Structure After Document Removal:\")\n",
    "    print_tree(tree.root)\n",
    "\n",
    "    # Verify total_words consistency\n",
    "    total_words_after_removal = total_words - len(corpus[1])\n",
    "    tree_total_words_after_removal = sum_total_words(tree.root)\n",
    "    print(f\"\\nTotal Words in Tree: {tree_total_words_after_removal} (Expected: {total_words_after_removal})\")\n",
    "    assert tree_total_words_after_removal == total_words_after_removal, \"Total words mismatch after removal!\"\n",
    "\n",
    "    print(\"\\nAdding a new document (Document 3) with words [1, 2]:\")\n",
    "    new_doc = [1, 2]\n",
    "    doc_id = len(corpus)\n",
    "    # Initialize path for the new document\n",
    "    path_nodes = tree.initialize_new_path(max_depth=3, document_id=doc_id)\n",
    "    # Assign levels to words\n",
    "    level_word_counts = {0: {1:1, 2:1}}  # Assigning both words to level 0\n",
    "    # Add the document to the tree\n",
    "    tree.add_document(document_id=doc_id, path_nodes=path_nodes, level_word_counts=level_word_counts)\n",
    "\n",
    "    print(\"\\nTree Structure After Adding Document 3:\")\n",
    "    print_tree(tree.root)\n",
    "\n",
    "    # Verify total_words consistency\n",
    "    total_words_final = total_words_after_removal + len(new_doc)\n",
    "    tree_total_words_final = sum_total_words(tree.root)\n",
    "    print(f\"\\nTotal Words in Tree: {tree_total_words_final} (Expected: {total_words_final})\")\n",
    "    assert tree_total_words_final == total_words_final, \"Total words mismatch after adding a new document!\"\n",
    "\n",
    "    print(\"\\nAttempting to remove a non-existent document (Document 999)...\")\n",
    "    tree.remove_document(document_id=999)\n",
    "\n",
    "    print(\"\\nTree Structure After Attempting to Remove Non-Existent Document:\")\n",
    "    print_tree(tree.root)\n",
    "\n",
    "    # Verify total_words consistency remains unchanged\n",
    "    tree_total_words_final_after_invalid_removal = sum_total_words(tree.root)\n",
    "    print(f\"\\nTotal Words in Tree: {tree_total_words_final_after_invalid_removal} (Expected: {total_words_final})\")\n",
    "    assert tree_total_words_final_after_invalid_removal == total_words_final, \"Total words changed after attempting to remove a non-existent document!\"\n",
    "\n",
    "    print(\"\\nAll test cases passed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA3288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
