{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Restaurant Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implementation is based on the formula below:\n",
    "\n",
    "![Image Description](../image/CRP%20formula.png)\n",
    "\n",
    "Note that in the following implementation, -1 in the denomenator is omitting since python is 0-based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRP(gamma, N):\n",
    "    \"\"\"\n",
    "    Simulate the Chinese Restaurant Process (CRP).\n",
    "\n",
    "    Parameters:\n",
    "    - gamma: concentration parameter. Measures how likely it is for a new table. Range = [0,infinity). \n",
    "    - N: the number of customers to seat.\n",
    "\n",
    "    Returns:\n",
    "    - table: A list representing the assignment of customers to tables.\n",
    "    - probability: Probability of sitting at each table\n",
    "    \"\"\"\n",
    "    tables = np.zeros(N)\n",
    "    tables[0] = 1 # root \n",
    "    \n",
    "    for i in range(1, N):# index is this way to ensure formula is calculated corretly\n",
    "        \n",
    "        prob_existing = tables[:i] / (i + gamma) # probability of joining an existing table.\n",
    "        prob_existing = prob_existing[prob_existing > 0] \n",
    "        prob_new = gamma / (i + gamma) # probability of creating a new table\n",
    "        \n",
    "        probability = np.append(prob_existing, prob_new)\n",
    "        \n",
    "        table_num = np.random.choice(len(probability), p = probability) # making selection\n",
    "        \n",
    "        if table_num == i:\n",
    "            tables[i] = 1  # new table\n",
    "        else:\n",
    "            tables[table_num] += 1 # existing table chosen, customer + 1\n",
    "            \n",
    "    tables = tables[tables > 0] # removing empty table entries\n",
    "    \n",
    "    return (tables, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7., 2., 1.]), array([0.6, 0.2, 0.1, 0.1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRP(1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Gibbs Sampler from Griffiths & Steyvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is designed for flat topic modeling as in traditional LDA. It assigns a single topic to each word in a document from a predefined number of topics. The output is the topic assignment for each word, and topics are treated independently of each other. There is no hierarchical structure.\n",
    "\n",
    "- Flat Topic Model: Each document is a mixture of topics, but the topics themselves donâ€™t have relationships with each other (i.e., no hierarchy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z(corpus, T, alpha, beta, num_iterations):\n",
    "    \"\"\"\n",
    "    Implements Gibbs sampling for topic assignment using the method from Griffiths & Steyvers.\n",
    "    \n",
    "    Parameters:\n",
    "    - corpus: A list of documents, where each document is a list of words.\n",
    "    - T: Number of topics.\n",
    "    - alpha: Dirichlet prior for document-topic distributions.\n",
    "    - beta: Dirichlet prior for topic-word distributions.\n",
    "    - num_iterations: Number of iterations to run the Gibbs sampler.\n",
    "    \n",
    "    Returns:\n",
    "    - z: Topic assignments for each word in each document. A list of list.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    unique_words_in_corpus = set(word for doc in corpus for word in doc)  # Set of unique words\n",
    "    word_to_index = {word: idx for idx, word in enumerate(unique_words_in_corpus)}  # Map words to indices\n",
    "    \n",
    "    D = len(corpus)                         # number of documents  \n",
    "    unique_words_in_corpus = set(word for doc in corpus for word in doc)\n",
    "    W = len(unique_words_in_corpus)         # unique vocabulary size of the entire corpus\n",
    "    \n",
    "    z = [[np.random.randint(0, T-1) for _ in doc] for doc in corpus]  # random initial topic assignments\n",
    "    \n",
    "    # Initialize count matrices\n",
    "    n_wt = np.zeros((T, W))  # word-topic counts\n",
    "    n_dt = np.zeros((D, T))  # document-topic counts\n",
    "    n_t = np.zeros(T)        # total word counts for each topic\n",
    "    \n",
    "    # Populate initial counts\n",
    "    for d, doc in enumerate(corpus):\n",
    "        for i, word in enumerate(doc):\n",
    "            word_idx = word_to_index[word]  # Convert word to its index\n",
    "            topic = z[d][i]\n",
    "            n_wt[topic][word_idx] += 1\n",
    "            n_dt[d][topic] += 1\n",
    "            n_t[topic] += 1\n",
    "    \n",
    "    # Gibbs sampling iterations\n",
    "    for iteration in range(num_iterations):\n",
    "        for d, doc in enumerate(corpus):\n",
    "            for i, word in enumerate(doc):\n",
    "                word_idx = word_to_index[word]  # Convert word to its index\n",
    "                current_topic = z[d][i]\n",
    "                \n",
    "                # Decrease current word counts\n",
    "                n_wt[current_topic][word_idx] -= 1\n",
    "                n_dt[d][current_topic] -= 1\n",
    "                n_t[current_topic] -= 1\n",
    "                \n",
    "                # Compute topic probabilities for this word\n",
    "                p = np.zeros(T)\n",
    "                for t in range(T):\n",
    "                    word_topic_prob = (n_wt[t][word_idx] + beta) / (n_t[t] + W * beta)\n",
    "                    doc_topic_prob = (n_dt[d][t] + alpha) / (len(doc) + T * alpha)\n",
    "                    p[t] = word_topic_prob * doc_topic_prob\n",
    "                \n",
    "                # Normalize and sample a new topic\n",
    "                p /= np.sum(p)\n",
    "                new_topic = np.random.choice(T, p=p)\n",
    "                \n",
    "                # Update counts with the new topic\n",
    "                z[d][i] = new_topic\n",
    "                n_wt[new_topic][word_idx] += 1\n",
    "                n_dt[d][new_topic] += 1\n",
    "                n_t[new_topic] += 1\n",
    "    \n",
    "    return z\n",
    "# Change z to a better representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0], [0, 0], [1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "corpus = [['apple', 'banana', 'apple'],\n",
    "          ['banana', 'cherry'],\n",
    "          ['math','art','science']]\n",
    "\n",
    "print(Z(corpus,2,0.1,0.1,100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA3288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
