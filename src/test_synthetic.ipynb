{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hlda_utils import *\n",
    "from hlda_final import *\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw documents in 20newsgroups (train): 11314\n",
      "Documents after filtering: 10999\n",
      "Vocabulary size: 1998\n",
      "Preprocessing complete.\n",
      "Vocabulary size after min_freq=100: 1998\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# LOAD & PREPROCESS 20 NEWSGROUPS\n",
    "###########################################\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
    "all_docs = dataset.data\n",
    "all_labels = dataset.target\n",
    "\n",
    "print(\"Total raw documents in 20newsgroups (train):\", len(all_docs))\n",
    "\n",
    "_, _, vocab, word2idx, idx2word, corpus_indices = \\\n",
    "    full_preprocessing_pipeline(all_docs, all_labels, \n",
    "                                min_word_length=2, \n",
    "                                min_freq=100)\n",
    "\n",
    "print(f\"Vocabulary size after min_freq=100: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected documents: 500 (each >= 100 tokens).\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# 1) SELECT 500 SUITABLY-LONG DOCS\n",
    "###########################################\n",
    "\n",
    "selected_corpus = []\n",
    "selected_count = 0\n",
    "for doc in corpus_indices:\n",
    "    if len(doc) >= 100:\n",
    "        selected_corpus.append(doc)\n",
    "        selected_count += 1\n",
    "        if selected_count == 500:\n",
    "            break\n",
    "\n",
    "print(f\"Number of selected documents: {len(selected_corpus)} (each >= 100 tokens).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hierarchical LDA sampling...\n",
      "Iteration 5000\n",
      "Topic Node 0 (level=0, docs=500): to(2214), it(1679), is(1625), you(1553), that(1310)\n",
      "    Topic Node 1 (level=1, docs=260): the(3467), of(1595), in(1084), and(1073), to(952)\n",
      "        Topic Node 2 (level=2, docs=195): the(1943), of(1230), and(769), that(746), to(737)\n",
      "        Topic Node 6 (level=2, docs=3): period(42), pp(37), play(30), power(29), pt(27)\n",
      "        Topic Node 11 (level=2, docs=30): the(205), of(54), is(50), chip(49), and(49)\n",
      "        Topic Node 14 (level=2, docs=10): the(85), of(85), and(79), turkish(31), presid(21)\n",
      "        Topic Node 72 (level=2, docs=6): of(48), and(46), for(36), in(29), sequenc(24)\n",
      "        Topic Node 286 (level=2, docs=13): the(26), run(22), program(21), have(18), window(18)\n",
      "        Topic Node 8262 (level=2, docs=3): may(20), ma(16), june(13), april(9), me(9)\n",
      "    Topic Node 3 (level=1, docs=166): the(2282), of(1048), and(644), in(573), to(550)\n",
      "        Topic Node 4 (level=2, docs=82): the(973), and(690), they(517), wa(454), that(404)\n",
      "        Topic Node 5 (level=2, docs=30): the(264), he(117), in(93), and(74), team(70)\n",
      "        Topic Node 15 (level=2, docs=4): of(125), and(100), health(59), use(50), in(41)\n",
      "        Topic Node 51 (level=2, docs=27): the(116), and(43), system(28), my(22), for(21)\n",
      "        Topic Node 1079 (level=2, docs=15): my(17), for(16), power(13), batteri(13), use(12)\n",
      "        Topic Node 6676 (level=2, docs=4): vote(40), will(25), for(20), be(18), your(17)\n",
      "        Topic Node 8787 (level=2, docs=2): washington(19), street(15), dc(14), new(14), york(13)\n",
      "        Topic Node 8859 (level=2, docs=2): den(52), doug(24), doubl(14), symbol(11), sin(8)\n",
      "    Topic Node 7 (level=1, docs=36): the(556), of(354), and(280), to(160), for(105)\n",
      "        Topic Node 8 (level=2, docs=15): the(164), is(151), in(107), on(101), and(87)\n",
      "        Topic Node 9 (level=2, docs=8): and(58), in(55), will(47), launch(35), mission(34)\n",
      "        Topic Node 201 (level=2, docs=11): the(26), none(25), in(22), by(21), kill(14)\n",
      "        Topic Node 8864 (level=2, docs=2): max(354), ql(62), giz(57), bhj(43), wm(30)\n",
      "    Topic Node 12 (level=1, docs=38): the(895), of(361), and(212), to(211), is(176)\n",
      "        Topic Node 18 (level=2, docs=4): shuttl(25), orbit(15), option(14), space(14), flight(13)\n",
      "        Topic Node 2023 (level=2, docs=3): is(124), argument(68), that(43), conclus(43), be(38)\n",
      "        Topic Node 3599 (level=2, docs=2): father(44), son(35), from(33), spirit(32), holi(22)\n",
      "        Topic Node 3646 (level=2, docs=10): for(19), game(18), disk(17), guid(13), your(12)\n",
      "        Topic Node 6214 (level=2, docs=7): circuit(8), audio(8), electron(7), switch(7), resist(7)\n",
      "        Topic Node 8548 (level=2, docs=2): good(48), veri(35), excel(33), miss(26), modul(18)\n",
      "        Topic Node 8601 (level=2, docs=7): printer(10), mous(6), would(5), ride(5), tax(5)\n",
      "        Topic Node 8845 (level=2, docs=3): pitcher(14), mon(12), cub(11), stl(11), lead(10)\n",
      "Total topic nodes created = 32\n"
     ]
    }
   ],
   "source": [
    "ground_truth_model = hLDA(\n",
    "    corpus=selected_corpus,\n",
    "    vocabulary=vocab,\n",
    "    L=3,\n",
    "    alpha=10.0,\n",
    "    gamma=1.0,\n",
    "    eta=0.1,  \n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "time = ground_truth_model.gibbs_sampling(\n",
    "    iterations=5000,\n",
    "    display_interval=5000,\n",
    "    top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average document length in selected corpus = 321 tokens\n",
      "Generated 500 synthetic documents, each ~321 words\n"
     ]
    }
   ],
   "source": [
    "doc_lengths = [len(doc) for doc in selected_corpus]\n",
    "avg_length = int(np.mean(doc_lengths))\n",
    "print(f\"Average document length in selected corpus = {avg_length} tokens\")\n",
    "\n",
    "# Generate 100 synthetic documents using the ground truth model\n",
    "num_synth_docs = 500\n",
    "synth_corpus = ground_truth_model.generate_synthetic_corpus(\n",
    "    num_docs=num_synth_docs,\n",
    "    words_per_doc=avg_length,\n",
    "    alpha_dir=10.0,   # or any other Dirichlet parameter you prefer\n",
    "    rng=None          # uses the model's default RNG\n",
    ")\n",
    "print(f\"Generated {len(synth_corpus)} synthetic documents, each ~{avg_length} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hierarchical LDA sampling...\n",
      "Iteration 5000\n",
      "Topic Node 0 (level=0, docs=500): the(3603), to(2635), and(1782), is(1695), it(1647)\n",
      "    Topic Node 1 (level=1, docs=433): the(3506), of(2216), and(1355), in(1268), to(1134)\n",
      "        Topic Node 2 (level=2, docs=41): vote(123), your(73), will(65), be(65), the(58)\n",
      "        Topic Node 3 (level=2, docs=25): the(156), of(130), and(115), turkish(51), presid(37)\n",
      "        Topic Node 6 (level=2, docs=331): the(2607), is(1198), of(905), to(821), that(615)\n",
      "        Topic Node 9 (level=2, docs=19): period(137), pp(111), pt(94), play(92), power(82)\n",
      "        Topic Node 13 (level=2, docs=11): none(41), kill(18), of(18), were(17), attack(15)\n",
      "        Topic Node 22 (level=2, docs=6): lo(21), lead(16), cub(15), pitcher(14), stl(13)\n",
      "    Topic Node 4 (level=1, docs=8): ad(8), two(7), pressur(7), were(5), issu(5)\n",
      "        Topic Node 23 (level=2, docs=8): the(83), good(53), veri(48), excel(47), cover(26)\n",
      "    Topic Node 7 (level=1, docs=18): the(219), of(91), in(46), and(37), wa(29)\n",
      "        Topic Node 8 (level=2, docs=11): den(140), doug(58), symbol(41), doubl(38), that(22)\n",
      "        Topic Node 15 (level=2, docs=7): printer(21), jet(14), especi(11), ball(10), ride(9)\n",
      "    Topic Node 16 (level=1, docs=32): the(270), of(200), and(123), to(62), is(50)\n",
      "        Topic Node 18 (level=2, docs=11): den(100), doug(52), doubl(31), sin(23), symbol(23)\n",
      "        Topic Node 21 (level=2, docs=21): max(709), ql(132), giz(124), bhj(81), wm(71)\n",
      "    Topic Node 27 (level=1, docs=9): new(12), guid(9), newsgroup(8), basic(7), did(7)\n",
      "        Topic Node 28 (level=2, docs=9): the(51), game(28), disk(20), of(18), softwar(16)\n",
      "Total topic nodes created = 18\n",
      "Synthetic-data model trained in ~442.9 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synthetic_model = hLDA(\n",
    "    corpus=synth_corpus,\n",
    "    vocabulary=vocab,\n",
    "    L=3,\n",
    "    alpha=10.0,\n",
    "    gamma=1.0,\n",
    "    eta=0.1,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "time_synth = synthetic_model.gibbs_sampling(\n",
    "    iterations=5000,\n",
    "    display_interval=5000,\n",
    "    top_n=5,\n",
    "    show=True\n",
    ")\n",
    "print(f\"Synthetic-data model trained in ~{time_synth} minutes.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ground Truth Model Structure (brief) ---\n",
      "Topic Node 0 (level=0, children=4)\n",
      "  Topic Node 1 (level=1, children=7)\n",
      "    Topic Node 2 (level=2, children=0)\n",
      "    Topic Node 6 (level=2, children=0)\n",
      "    Topic Node 11 (level=2, children=0)\n",
      "    Topic Node 14 (level=2, children=0)\n",
      "    Topic Node 72 (level=2, children=0)\n",
      "    Topic Node 286 (level=2, children=0)\n",
      "    Topic Node 8262 (level=2, children=0)\n",
      "  Topic Node 3 (level=1, children=8)\n",
      "    Topic Node 4 (level=2, children=0)\n",
      "    Topic Node 5 (level=2, children=0)\n",
      "    Topic Node 15 (level=2, children=0)\n",
      "    Topic Node 51 (level=2, children=0)\n",
      "    Topic Node 1079 (level=2, children=0)\n",
      "    Topic Node 6676 (level=2, children=0)\n",
      "    Topic Node 8787 (level=2, children=0)\n",
      "    Topic Node 8859 (level=2, children=0)\n",
      "  Topic Node 7 (level=1, children=4)\n",
      "    Topic Node 8 (level=2, children=0)\n",
      "    Topic Node 9 (level=2, children=0)\n",
      "    Topic Node 201 (level=2, children=0)\n",
      "    Topic Node 8864 (level=2, children=0)\n",
      "  Topic Node 12 (level=1, children=8)\n",
      "    Topic Node 18 (level=2, children=0)\n",
      "    Topic Node 2023 (level=2, children=0)\n",
      "    Topic Node 3599 (level=2, children=0)\n",
      "    Topic Node 3646 (level=2, children=0)\n",
      "    Topic Node 6214 (level=2, children=0)\n",
      "    Topic Node 8548 (level=2, children=0)\n",
      "    Topic Node 8601 (level=2, children=0)\n",
      "    Topic Node 8845 (level=2, children=0)\n",
      "\n",
      "--- New Synthetic Model Structure (brief) ---\n",
      "Topic Node 0 (level=0, children=5)\n",
      "  Topic Node 1 (level=1, children=6)\n",
      "    Topic Node 2 (level=2, children=0)\n",
      "    Topic Node 3 (level=2, children=0)\n",
      "    Topic Node 6 (level=2, children=0)\n",
      "    Topic Node 9 (level=2, children=0)\n",
      "    Topic Node 13 (level=2, children=0)\n",
      "    Topic Node 22 (level=2, children=0)\n",
      "  Topic Node 4 (level=1, children=1)\n",
      "    Topic Node 23 (level=2, children=0)\n",
      "  Topic Node 7 (level=1, children=2)\n",
      "    Topic Node 8 (level=2, children=0)\n",
      "    Topic Node 15 (level=2, children=0)\n",
      "  Topic Node 16 (level=1, children=2)\n",
      "    Topic Node 18 (level=2, children=0)\n",
      "    Topic Node 21 (level=2, children=0)\n",
      "  Topic Node 27 (level=1, children=1)\n",
      "    Topic Node 28 (level=2, children=0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Ground Truth Model Structure (brief) ---\")\n",
    "ground_truth_model.exhibit_topics(top_n=5, structure=True)\n",
    "\n",
    "print(\"\\n--- New Synthetic Model Structure (brief) ---\")\n",
    "synthetic_model.exhibit_topics(top_n=5, structure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average document length in selected corpus = 321 tokens\n",
      "Generated 500 synthetic documents, each ~321 words\n"
     ]
    }
   ],
   "source": [
    "doc_lengths = [len(doc) for doc in selected_corpus]\n",
    "avg_length = int(np.mean(doc_lengths))\n",
    "print(f\"Average document length in selected corpus = {avg_length} tokens\")\n",
    "\n",
    "# Generate 100 synthetic documents using the ground truth model\n",
    "num_synth_docs = 100\n",
    "synth_corpus1 = ground_truth_model.generate_synthetic_corpus(\n",
    "    num_docs=num_synth_docs,\n",
    "    words_per_doc=avg_length,\n",
    "    alpha_dir=10.0,   # or any other Dirichlet parameter you prefer\n",
    "    rng=None          # uses the model's default RNG\n",
    ")\n",
    "print(f\"Generated {len(synth_corpus)} synthetic documents, each ~{avg_length} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hierarchical LDA sampling...\n",
      "Iteration 5000\n",
      "Topic Node 0 (level=0, docs=500): the(3786), to(2604), is(1768), and(1537), for(1497)\n",
      "    Topic Node 1 (level=1, docs=430): the(3460), of(2385), and(1558), in(1429), to(1104)\n",
      "        Topic Node 2 (level=2, docs=33): vote(124), will(68), be(61), your(59), the(51)\n",
      "        Topic Node 3 (level=2, docs=24): of(143), and(108), the(104), turkish(49), presid(41)\n",
      "        Topic Node 6 (level=2, docs=335): the(2518), is(1131), of(943), to(827), it(559)\n",
      "        Topic Node 9 (level=2, docs=19): period(137), pp(111), pt(93), play(90), power(77)\n",
      "        Topic Node 13 (level=2, docs=15): of(65), and(60), none(44), were(21), kill(20)\n",
      "        Topic Node 22 (level=2, docs=4): lo(21), lead(15), cub(15), pitcher(14), stl(13)\n",
      "    Topic Node 4 (level=1, docs=7): orient(6), two(5), howev(5), edit(5), is(4)\n",
      "        Topic Node 23 (level=2, docs=7): the(58), good(49), excel(48), veri(45), of(33)\n",
      "    Topic Node 7 (level=1, docs=16): the(182), of(94), and(62), in(40), to(33)\n",
      "        Topic Node 8 (level=2, docs=10): den(140), doug(58), symbol(40), doubl(38), with(17)\n",
      "        Topic Node 15 (level=2, docs=6): printer(21), jet(14), especi(10), clean(9), tax(9)\n",
      "    Topic Node 16 (level=1, docs=36): the(309), of(250), and(142), to(46), is(44)\n",
      "        Topic Node 18 (level=2, docs=15): den(101), doug(52), doubl(33), symbol(23), sin(23)\n",
      "        Topic Node 21 (level=2, docs=21): max(711), ql(132), giz(126), bhj(81), wm(71)\n",
      "    Topic Node 27 (level=1, docs=11): game(19), use(17), disk(15), modem(14), new(14)\n",
      "        Topic Node 28 (level=2, docs=11): the(80), of(72), and(30), year(10), more(8)\n",
      "Total topic nodes created = 29\n",
      "Synthetic-data model trained in ~442.9 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synthetic_model1 = hLDA(\n",
    "    corpus=synth_corpus,\n",
    "    vocabulary=vocab,\n",
    "    L=3,\n",
    "    alpha=10.0,\n",
    "    gamma=1.0,\n",
    "    eta=0.1,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "time_synth1 = synthetic_model.gibbs_sampling(\n",
    "    iterations=5000,\n",
    "    display_interval=5000,\n",
    "    top_n=5,\n",
    "    show=True\n",
    ")\n",
    "print(f\"Synthetic-data model trained in ~{time_synth} minutes.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ground Truth Model Structure (brief) ---\n",
      "Topic Node 0 (level=0, children=4)\n",
      "  Topic Node 1 (level=1, children=7)\n",
      "    Topic Node 2 (level=2, children=0)\n",
      "    Topic Node 6 (level=2, children=0)\n",
      "    Topic Node 11 (level=2, children=0)\n",
      "    Topic Node 14 (level=2, children=0)\n",
      "    Topic Node 72 (level=2, children=0)\n",
      "    Topic Node 286 (level=2, children=0)\n",
      "    Topic Node 8262 (level=2, children=0)\n",
      "  Topic Node 3 (level=1, children=8)\n",
      "    Topic Node 4 (level=2, children=0)\n",
      "    Topic Node 5 (level=2, children=0)\n",
      "    Topic Node 15 (level=2, children=0)\n",
      "    Topic Node 51 (level=2, children=0)\n",
      "    Topic Node 1079 (level=2, children=0)\n",
      "    Topic Node 6676 (level=2, children=0)\n",
      "    Topic Node 8787 (level=2, children=0)\n",
      "    Topic Node 8859 (level=2, children=0)\n",
      "  Topic Node 7 (level=1, children=4)\n",
      "    Topic Node 8 (level=2, children=0)\n",
      "    Topic Node 9 (level=2, children=0)\n",
      "    Topic Node 201 (level=2, children=0)\n",
      "    Topic Node 8864 (level=2, children=0)\n",
      "  Topic Node 12 (level=1, children=8)\n",
      "    Topic Node 18 (level=2, children=0)\n",
      "    Topic Node 2023 (level=2, children=0)\n",
      "    Topic Node 3599 (level=2, children=0)\n",
      "    Topic Node 3646 (level=2, children=0)\n",
      "    Topic Node 6214 (level=2, children=0)\n",
      "    Topic Node 8548 (level=2, children=0)\n",
      "    Topic Node 8601 (level=2, children=0)\n",
      "    Topic Node 8845 (level=2, children=0)\n",
      "\n",
      "--- New Synthetic Model Structure (brief) ---\n",
      "Topic Node 0 (level=0, children=6)\n",
      "  Topic Node 1 (level=1, children=8)\n",
      "    Topic Node 2 (level=2, children=0)\n",
      "    Topic Node 3 (level=2, children=0)\n",
      "    Topic Node 6 (level=2, children=0)\n",
      "    Topic Node 9 (level=2, children=0)\n",
      "    Topic Node 13 (level=2, children=0)\n",
      "    Topic Node 14 (level=2, children=0)\n",
      "    Topic Node 22 (level=2, children=0)\n",
      "    Topic Node 25 (level=2, children=0)\n",
      "  Topic Node 4 (level=1, children=4)\n",
      "    Topic Node 5 (level=2, children=0)\n",
      "    Topic Node 20 (level=2, children=0)\n",
      "    Topic Node 23 (level=2, children=0)\n",
      "    Topic Node 26 (level=2, children=0)\n",
      "  Topic Node 7 (level=1, children=5)\n",
      "    Topic Node 8 (level=2, children=0)\n",
      "    Topic Node 12 (level=2, children=0)\n",
      "    Topic Node 15 (level=2, children=0)\n",
      "    Topic Node 19 (level=2, children=0)\n",
      "    Topic Node 24 (level=2, children=0)\n",
      "  Topic Node 10 (level=1, children=1)\n",
      "    Topic Node 11 (level=2, children=0)\n",
      "  Topic Node 16 (level=1, children=3)\n",
      "    Topic Node 17 (level=2, children=0)\n",
      "    Topic Node 18 (level=2, children=0)\n",
      "    Topic Node 21 (level=2, children=0)\n",
      "  Topic Node 27 (level=1, children=1)\n",
      "    Topic Node 28 (level=2, children=0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Ground Truth Model Structure (brief) ---\")\n",
    "ground_truth_model.exhibit_topics(top_n=5, structure=True)\n",
    "\n",
    "print(\"\\n--- New Synthetic Model Structure (brief) ---\")\n",
    "synthetic_model1.exhibit_topics(top_n=5, structure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Model**       | **Total Nodes** | **Nodes at Level 2** | **Nodes at Level 3** | **No. of Documents** |\n",
    "|-----------------|-----------------|----------------------|----------------------|----------------------|\n",
    "| \"Ground Truth\"    | 32              | 4                    | 27                   | 500                  |\n",
    "| Synthetic 1     | 18              | 5                    | 12                   | 500                  |\n",
    "| Synthetic 2     | 30              | 6                    | 22                   | 100                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"cat\",\"dog\",\"mouse\",\"pizza\",\"music\",\"food\",\"film\",\"code\"]\n",
    "docs = [\n",
    "    [0,0,1,1,6,7],  # \"cat cat dog dog film code\"\n",
    "    [3,3,4,4,5,2],  # \"pizza pizza music music food mouse\"\n",
    "    [0,3,4,1,7,7],  # mixture of cat,pizza,music,dog,code...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hierarchical LDA sampling...\n",
      "Iteration 10\n",
      "Topic Node 0 (level=0, docs=3): code(3), dog(3), mouse(1)\n",
      "    Topic Node 4 (level=1, docs=2): music(3), pizza(3), food(1)\n",
      "    Topic Node 5 (level=1, docs=1): cat(2), film(1), code(0)\n",
      "Iteration 20\n",
      "Topic Node 0 (level=0, docs=3): music(3), pizza(3), dog(3)\n",
      "    Topic Node 4 (level=1, docs=3): code(3), cat(3), mouse(1)\n",
      "Total topic nodes created = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = hLDA(\n",
    "    corpus=docs,\n",
    "    vocabulary=vocab,\n",
    "    L=2,      # root + 1 child level\n",
    "    alpha=5.0,\n",
    "    gamma=1.0,\n",
    "    eta=0.1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "model.gibbs_sampling(iterations=20, display_interval=10, top_n=3, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tree Structure (structure=True) ---\n",
      "Topic Node 0 (level=0, children=1)\n",
      "  Topic Node 4 (level=1, children=0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Tree Structure (structure=True) ---\")\n",
    "model.exhibit_topics(structure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Synthetic Docs ---\n",
      "Doc 0: ['cat', 'film', 'pizza', 'code', 'code', 'dog']\n",
      "Doc 1: ['pizza', 'pizza', 'dog', 'music', 'food', 'cat']\n",
      "Doc 2: ['film', 'food', 'code', 'music', 'mouse', 'music']\n"
     ]
    }
   ],
   "source": [
    "syn_corpus = model.generate_synthetic_corpus(\n",
    "    num_docs=3,          # create 3 new docs\n",
    "    words_per_doc=6,     # each doc has 6 words\n",
    "    alpha_dir=3.0,       # Dirichlet parameter for doc-level mixing\n",
    "    rng=None             # uses model's default random state\n",
    ")\n",
    "print(\"\\n--- Synthetic Docs ---\")\n",
    "for i, sdoc in enumerate(syn_corpus):\n",
    "    text_tokens = [vocab[w] for w in sdoc]\n",
    "    print(f\"Doc {i}: {text_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA3288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
