{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from functools import lru_cache\n",
    "from scipy.special import gammaln\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Represents a node in the hierarchical tree for hLDA.\n",
    "    \"\"\"\n",
    "    def __init__(self, parent=None, level=0):\n",
    "        self.children = {}\n",
    "        self.documents = 0\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.total_words = 0\n",
    "        self.parent = parent\n",
    "        self.level = level\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def add_child(self, topic_id):\n",
    "        child_node = Node(parent=self, level=self.level + 1)\n",
    "        self.children[topic_id] = child_node\n",
    "        return child_node\n",
    "\n",
    "    def remove_child(self, topic_id):\n",
    "        if topic_id in self.children:\n",
    "            del self.children[topic_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nCRPTree:\n",
    "    \"\"\"\n",
    "    Implements the hierarchical LDA model using the nested Chinese Restaurant Process.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, eta, num_levels, vocab, m=0.5, pi=1.0):\n",
    "        self.root = Node()\n",
    "        self.gamma = gamma\n",
    "        self.eta = eta\n",
    "        self.V = len(vocab)\n",
    "        self.eta_sum = self.eta * self.V\n",
    "        self.num_levels = num_levels\n",
    "        self.paths = {}\n",
    "        self.levels = {}\n",
    "        self.document_words = {}\n",
    "        self.m = m\n",
    "        self.pi = pi\n",
    "        self.vocab = vocab\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_gammaln(self, x):\n",
    "        return gammaln(x)\n",
    "\n",
    "    def sample_ncrp_path(self, node):\n",
    "        total_customers = node.documents\n",
    "        topic_probabilities = {}\n",
    "\n",
    "        # Existing children probabilities\n",
    "        for topic_id, child in node.children.items():\n",
    "            topic_probabilities[topic_id] = child.documents / (total_customers + self.gamma)\n",
    "\n",
    "        # New child probability\n",
    "        new_topic_key = max(node.children.keys(), default=0) + 1\n",
    "        topic_probabilities[new_topic_key] = self.gamma / (total_customers + self.gamma)\n",
    "\n",
    "        topics = list(topic_probabilities.keys())\n",
    "        probs = np.array(list(topic_probabilities.values()))\n",
    "        probs /= probs.sum()\n",
    "        chosen = np.random.choice(topics, p=probs)\n",
    "        is_new = (chosen not in node.children)\n",
    "        return chosen, is_new\n",
    "\n",
    "    def initialize_new_path(self, max_depth, document_id):\n",
    "        current_node = self.root\n",
    "        current_node.documents += 1\n",
    "        path_nodes = [current_node]\n",
    "\n",
    "        for level in range(1, max_depth):\n",
    "            topic_id, is_new = self.sample_ncrp_path(current_node)\n",
    "            if is_new:\n",
    "                child_node = current_node.add_child(topic_id)\n",
    "            else:\n",
    "                child_node = current_node.children[topic_id]\n",
    "            child_node.documents += 1\n",
    "            path_nodes.append(child_node)\n",
    "            current_node = child_node\n",
    "\n",
    "        self.paths[document_id] = path_nodes\n",
    "        return path_nodes\n",
    "\n",
    "    def initialise_tree(self, corpus, max_depth):\n",
    "        for doc_id, doc_words in enumerate(corpus):\n",
    "            self.document_words[doc_id] = doc_words\n",
    "            path_nodes = self.initialize_new_path(max_depth, doc_id)\n",
    "            doc_levels = []\n",
    "            num_levels = len(path_nodes)\n",
    "            for w in doc_words:\n",
    "                level = np.random.randint(0, num_levels)\n",
    "                doc_levels.append(level)\n",
    "                node = path_nodes[level]\n",
    "                node.word_counts[w] += 1\n",
    "                node.total_words += 1\n",
    "            self.levels[doc_id] = doc_levels\n",
    "\n",
    "    def add_document(self, document_id, path_nodes, level_word_counts):\n",
    "        self.paths[document_id] = path_nodes\n",
    "        for node in path_nodes:\n",
    "            node.documents += 1\n",
    "        for level, w_counts in level_word_counts.items():\n",
    "            node = path_nodes[level]\n",
    "            for w, cnt in w_counts.items():\n",
    "                node.word_counts[w] += cnt\n",
    "                node.total_words += cnt\n",
    "\n",
    "    def remove_document(self, document_id):\n",
    "        if document_id not in self.paths:\n",
    "            return\n",
    "        path_nodes = self.paths[document_id]\n",
    "        doc_levels = self.levels[document_id]\n",
    "        doc_words = self.document_words[document_id]\n",
    "\n",
    "        # Decrement word counts\n",
    "        for w, lvl in zip(doc_words, doc_levels):\n",
    "            node = path_nodes[lvl]\n",
    "            node.word_counts[w] -= 1\n",
    "            if node.word_counts[w] == 0:\n",
    "                del node.word_counts[w]\n",
    "            node.total_words -= 1\n",
    "\n",
    "        # Decrement document counts\n",
    "        for node in path_nodes:\n",
    "            node.documents -= 1\n",
    "\n",
    "        # Prune empty leaves\n",
    "        for node in reversed(path_nodes):\n",
    "            if node.documents == 0 and node.is_leaf() and node.parent is not None:\n",
    "                parent = node.parent\n",
    "                remove_id = None\n",
    "                for tid, cnode in parent.children.items():\n",
    "                    if cnode == node:\n",
    "                        remove_id = tid\n",
    "                        break\n",
    "                if remove_id is not None:\n",
    "                    parent.remove_child(remove_id)\n",
    "\n",
    "        del self.paths[document_id]\n",
    "        del self.levels[document_id]\n",
    "        del self.document_words[document_id]\n",
    "\n",
    "    def get_level_word_counts(self, document_words, document_levels):\n",
    "        level_word_counts = {}\n",
    "        for w, lvl in zip(document_words, document_levels):\n",
    "            if lvl not in level_word_counts:\n",
    "                level_word_counts[lvl] = {}\n",
    "            level_word_counts[lvl][w] = level_word_counts[lvl].get(w, 0) + 1\n",
    "        return level_word_counts\n",
    "\n",
    "    def level_likelihood(self, node, M):\n",
    "        Nminus = node.word_counts\n",
    "        sumNminus = node.total_words\n",
    "        sumM = sum(M.values())\n",
    "        sumN = sumNminus + sumM\n",
    "\n",
    "        log_part1 = self.cached_gammaln(sumNminus + self.eta_sum)\n",
    "        for w_count in Nminus.values():\n",
    "            log_part1 -= self.cached_gammaln(w_count + self.eta)\n",
    "\n",
    "        log_part2 = 0.0\n",
    "        for w, n_w_minus in Nminus.items():\n",
    "            n_w = n_w_minus + M.get(w, 0)\n",
    "            log_part2 += self.cached_gammaln(n_w + self.eta)\n",
    "        for w, mcount in M.items():\n",
    "            if w not in Nminus:\n",
    "                log_part2 += self.cached_gammaln(mcount + self.eta)\n",
    "\n",
    "        log_part2 -= self.cached_gammaln(sumN + self.eta_sum)\n",
    "\n",
    "        return log_part1 + log_part2\n",
    "\n",
    "    def level_prior(self, parent, child_is_new, child_node=None):\n",
    "        total_customers = parent.documents\n",
    "        if child_is_new:\n",
    "            return log(self.gamma) - log(total_customers + self.gamma)\n",
    "        else:\n",
    "            return log(child_node.documents) - log(total_customers + self.gamma)\n",
    "\n",
    "    def sample_path_level(self, parent_node, level_word_counts, level_index):\n",
    "        candidates = list(parent_node.children.items())  # (topic_id, node)\n",
    "        new_topic_id = max(parent_node.children.keys(), default=0) + 1\n",
    "\n",
    "        M = level_word_counts.get(level_index, {})\n",
    "\n",
    "        # Existing children\n",
    "        log_probs = []\n",
    "        for topic_id, child_node in candidates:\n",
    "            lp = self.level_prior(parent_node, False, child_node)\n",
    "            lp += self.level_likelihood(child_node, M)\n",
    "            log_probs.append((lp, topic_id, False))\n",
    "\n",
    "        # New child\n",
    "        fake_node = Node(parent=parent_node, level=parent_node.level + 1)\n",
    "        lp_new = self.level_prior(parent_node, True)\n",
    "        lp_new += self.level_likelihood(fake_node, M)\n",
    "        log_probs.append((lp_new, new_topic_id, True))\n",
    "\n",
    "        lps = [x[0] for x in log_probs]\n",
    "        max_lp = max(lps)\n",
    "        weights = np.exp([lp - max_lp for lp in lps])\n",
    "        probs = weights / weights.sum()\n",
    "\n",
    "        chosen_index = np.random.choice(len(probs), p=probs)\n",
    "        chosen_lp, chosen_topic_id, chosen_is_new = log_probs[chosen_index]\n",
    "\n",
    "        if chosen_is_new:\n",
    "            child_node = parent_node.add_child(chosen_topic_id)\n",
    "        else:\n",
    "            child_node = parent_node.children[chosen_topic_id]\n",
    "\n",
    "        return child_node\n",
    "\n",
    "    def sample_path(self, document_id, document_words, document_levels):\n",
    "        if document_id in self.paths:\n",
    "            self.remove_document(document_id)\n",
    "\n",
    "        level_word_counts = self.get_level_word_counts(document_words, document_levels)\n",
    "        current_node = self.root\n",
    "        path_nodes = [current_node]\n",
    "\n",
    "        for ell in range(1, self.num_levels):\n",
    "            child_node = self.sample_path_level(current_node, level_word_counts, ell)\n",
    "            path_nodes.append(child_node)\n",
    "            current_node = child_node\n",
    "\n",
    "        self.add_document(document_id, path_nodes, level_word_counts)\n",
    "        self.levels[document_id] = document_levels\n",
    "        self.document_words[document_id] = document_words\n",
    "        max_level = max(document_levels) if document_levels else 0\n",
    "        assert max_level < self.num_levels, \"Document level assignments exceed maximum tree depth.\"\n",
    "\n",
    "    def compute_level_prior_probs(self, z_counts, max_z, m, pi):\n",
    "        sum_ge = [0]*(max_z+1)\n",
    "        running_sum = 0\n",
    "        for level_idx in range(max_z, -1, -1):\n",
    "            running_sum += z_counts[level_idx]\n",
    "            sum_ge[level_idx] = running_sum\n",
    "\n",
    "        probs = []\n",
    "        for k in range(max_z+1):\n",
    "            numerator = (m * pi + z_counts[k])\n",
    "            denominator = (pi + sum_ge[k]) if sum_ge[k] > 0 else pi\n",
    "            level_prob = numerator / denominator\n",
    "\n",
    "            for j in range(k):\n",
    "                numerator_j = ((1 - m) * pi + z_counts[j])\n",
    "                denominator_j = (pi + sum_ge[j]) if sum_ge[j] > 0 else pi\n",
    "                level_prob *= (numerator_j / denominator_j)\n",
    "            probs.append(level_prob)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def sample_level_assignment_for_word(self, document_id, n):\n",
    "        doc_words = self.document_words[document_id]\n",
    "        doc_levels = self.levels[document_id]\n",
    "        old_level = doc_levels[n]\n",
    "        w = doc_words[n]\n",
    "\n",
    "        path_nodes = self.paths[document_id]\n",
    "        old_node = path_nodes[old_level]\n",
    "        old_node.word_counts[w] -= 1\n",
    "        if old_node.word_counts[w] == 0:\n",
    "            del old_node.word_counts[w]\n",
    "        old_node.total_words -= 1\n",
    "\n",
    "        doc_levels[n] = -1\n",
    "        z_counts = defaultdict(int)\n",
    "        for lvl in doc_levels:\n",
    "            if lvl >= 0:\n",
    "                z_counts[lvl] += 1\n",
    "\n",
    "        max_z = max(z_counts.keys()) if z_counts else 0\n",
    "        level_range = list(range(max_z + 1))\n",
    "        z_counts_list = [z_counts.get(k, 0) for k in level_range]\n",
    "        prior_probs = self.compute_level_prior_probs(z_counts_list, max_z, self.m, self.pi)\n",
    "\n",
    "        # Word likelihood for existing levels\n",
    "        word_likelihoods = []\n",
    "        for k in level_range:\n",
    "            node = path_nodes[k]\n",
    "            w_count = node.word_counts.get(w, 0)\n",
    "            likelihood = (w_count + self.eta) / (node.total_words + self.eta_sum)\n",
    "            word_likelihoods.append(likelihood)\n",
    "\n",
    "        for i in range(len(prior_probs)):\n",
    "            prior_probs[i] *= word_likelihoods[i]\n",
    "\n",
    "        sum_existing = sum(prior_probs)\n",
    "        leftover = 1.0 - sum_existing\n",
    "        final_levels = level_range[:]\n",
    "        final_probs = prior_probs[:]\n",
    "\n",
    "        # If leftover > 0, consider going beyond max_z as per eq (3)\n",
    "        if leftover > 1e-15:\n",
    "            # We do a sequence of Bernoulli trials:\n",
    "            # For each deeper level ell > max_z:\n",
    "            # p(success) = (1-m)*p(w|...) with p(w|...) = eta/eta_sum for a new empty node\n",
    "            ell = max_z + 1\n",
    "            p_w_new = self.eta / self.eta_sum\n",
    "            # We must decide step-by-step:\n",
    "            # We know we are going beyond max_z, so attempt ell:\n",
    "            # Bernoulli trial with p=(1-m)*p_w_new:\n",
    "            # If success assign ell and stop.\n",
    "            # If fail, try ell+1, until we run out of levels.\n",
    "            # If we reach last level and still fail, assign the last tried level anyway.\n",
    "\n",
    "            chosen_level = None\n",
    "            while ell < self.num_levels:\n",
    "                p_success = (1 - self.m)*p_w_new\n",
    "                # Bernoulli trial:\n",
    "                success = (np.random.rand() < p_success)\n",
    "                if success:\n",
    "                    chosen_level = ell\n",
    "                    break\n",
    "                else:\n",
    "                    ell += 1\n",
    "\n",
    "            if chosen_level is None:\n",
    "                # If we fail all the way, assign the deepest level:\n",
    "                chosen_level = self.num_levels - 1\n",
    "\n",
    "            # Assign chosen_level now\n",
    "            new_level = chosen_level\n",
    "        else:\n",
    "            # Just choose from existing levels\n",
    "            total = sum_existing\n",
    "            if total == 0:\n",
    "                # rare fallback\n",
    "                new_level = max_z\n",
    "            else:\n",
    "                probs = [p/total for p in final_probs]\n",
    "                new_level = np.random.choice(final_levels, p=probs)\n",
    "\n",
    "        # Update counts\n",
    "        new_node = path_nodes[new_level]\n",
    "        new_node.word_counts[w] = new_node.word_counts.get(w,0) + 1\n",
    "        new_node.total_words += 1\n",
    "        doc_levels[n] = new_level\n",
    "\n",
    "    def sample_levels_for_document(self, document_id):\n",
    "        doc_words = self.document_words[document_id]\n",
    "        for n in range(len(doc_words)):\n",
    "            self.sample_level_assignment_for_word(document_id, n)\n",
    "\n",
    "    def gibbs_sampling(self, corpus, num_iterations, burn_in=100, thinning=10):\n",
    "        self.initialise_tree(corpus, max_depth=self.num_levels)\n",
    "        for it in range(num_iterations):\n",
    "            for doc_id in range(len(corpus)):\n",
    "                document_words = self.document_words[doc_id]\n",
    "                document_levels = self.levels[doc_id]\n",
    "                self.sample_path(doc_id, document_words, document_levels)\n",
    "                self.sample_levels_for_document(doc_id)\n",
    "\n",
    "            if (it + 1) % thinning == 0:\n",
    "                print(f\"Iteration {it + 1} completed.\")\n",
    "\n",
    "            if it + 1 == burn_in:\n",
    "                print(f\"Burn-in period of {burn_in} iterations completed.\")\n",
    "        print(\"Gibbs sampling completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(node, vocab, top_n=5):\n",
    "    word_counts = list(node.word_counts.items())\n",
    "    word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_words = [w for w, count in word_counts[:top_n]]\n",
    "    return top_words\n",
    "\n",
    "def print_tree(node, vocab, level=0):\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}Level {node.level}: docs={node.documents}, total_words={node.total_words}\")\n",
    "    top_words = get_top_words(node, vocab, top_n=5)\n",
    "    print(f\"{indent}  Top words: {top_words}\")\n",
    "    for child_id, child_node in node.children.items():\n",
    "        print_tree(child_node, vocab, level+1)\n",
    "\n",
    "def print_document_assignments(tree, doc_id):\n",
    "    doc_words = tree.document_words[doc_id]\n",
    "    doc_levels = tree.levels[doc_id]\n",
    "    print(f\"Document {doc_id}:\")\n",
    "    for w, lvl in zip(doc_words, doc_levels):\n",
    "        print(f\"  {w} -> level {lvl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree_graphviz(node, vocab, graph=None, parent_id=None, node_id_counter=None, label_map=None):\n",
    "    \"\"\"\n",
    "    Recursively traverse the tree and add nodes and edges to the Graphviz Digraph.\n",
    "\n",
    "    Args:\n",
    "        node (Node): The current node to visualize.\n",
    "        vocab (list): List of vocabulary words.\n",
    "        graph (Digraph, optional): The Graphviz Digraph object. Defaults to None.\n",
    "        parent_id (int, optional): The ID of the parent node. Defaults to None.\n",
    "        node_id_counter (list, optional): A single-element list acting as a mutable counter for node IDs. Defaults to None.\n",
    "        label_map (dict, optional): Mapping from node IDs to labels. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (graph, current_node_id, label_map)\n",
    "    \"\"\"\n",
    "    if graph is None:\n",
    "        graph = Digraph(comment='nCRP Tree')\n",
    "        graph.attr('node', shape='box', style='filled', color='lightblue')\n",
    "        label_map = {}\n",
    "    \n",
    "    if node_id_counter is None:\n",
    "        node_id_counter = [0]  # Initialize counter\n",
    "    \n",
    "    current_id = node_id_counter[0]\n",
    "    \n",
    "    # Create a label for the current node based on top words\n",
    "    top_words = sorted(node.word_counts.keys(), key=lambda w: node.word_counts[w], reverse=True)[:3]\n",
    "    label = f\"Level {node.level}\\nDocs: {node.documents}\\nWords: {', '.join(top_words)}\"\n",
    "    label_map[current_id] = label\n",
    "    graph.node(str(current_id), label=label)\n",
    "    \n",
    "    # Add edge from parent to current node\n",
    "    if parent_id is not None:\n",
    "        graph.edge(str(parent_id), str(current_id))\n",
    "    \n",
    "    # Traverse children\n",
    "    for child_topic_id, child_node in node.children.items():\n",
    "        node_id_counter[0] += 1  # Increment counter for the child\n",
    "        child_id = node_id_counter[0]\n",
    "        graph, node_id_counter, label_map = visualize_tree_graphviz(\n",
    "            child_node, vocab, graph, parent_id=current_id, node_id_counter=node_id_counter, label_map=label_map\n",
    "        )\n",
    "    \n",
    "    return graph, node_id_counter, label_map\n",
    "\n",
    "def print_tree_graphviz(root, vocab, filename='ncrp_tree', view=False):\n",
    "    \"\"\"\n",
    "    Generate and render the tree visualization using Graphviz.\n",
    "\n",
    "    Args:\n",
    "        root (Node): The root node of the tree.\n",
    "        vocab (list): List of vocabulary words.\n",
    "        filename (str, optional): Filename for the output. Defaults to 'ncrp_tree'.\n",
    "        view (bool, optional): Whether to automatically open the visualization. Defaults to False.\n",
    "    \"\"\"\n",
    "    graph, _, _ = visualize_tree_graphviz(root, vocab)\n",
    "    graph.render(filename, view=view, format='png')\n",
    "    print(f\"Tree visualization saved as {filename}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 completed.\n",
      "Burn-in period of 5 iterations completed.\n",
      "Iteration 10 completed.\n",
      "Iteration 15 completed.\n",
      "Iteration 20 completed.\n",
      "Gibbs sampling completed.\n",
      "=== Tree Structure After Gibbs Sampling ===\n",
      "Level 0: docs=4, total_words=0\n",
      "  Top words: []\n",
      "  Level 1: docs=2, total_words=0\n",
      "    Top words: []\n",
      "    Level 2: docs=1, total_words=3\n",
      "      Top words: ['banana', 'apple']\n",
      "    Level 2: docs=1, total_words=3\n",
      "      Top words: ['grape', 'apple']\n",
      "  Level 1: docs=2, total_words=0\n",
      "    Top words: []\n",
      "    Level 2: docs=1, total_words=3\n",
      "      Top words: ['apple', 'banana']\n",
      "    Level 2: docs=1, total_words=3\n",
      "      Top words: ['grape', 'banana', 'apple']\n",
      "\n",
      "=== Document Assignments ===\n",
      "Document 0:\n",
      "  apple -> level 2\n",
      "  banana -> level 2\n",
      "  apple -> level 2\n",
      "Document 1:\n",
      "  banana -> level 2\n",
      "  banana -> level 2\n",
      "  apple -> level 2\n",
      "Document 2:\n",
      "  apple -> level 2\n",
      "  grape -> level 2\n",
      "  grape -> level 2\n",
      "Document 3:\n",
      "  grape -> level 2\n",
      "  banana -> level 2\n",
      "  apple -> level 2\n",
      "Iteration 2 completed.\n",
      "Burn-in period of 2 iterations completed.\n",
      "Iteration 4 completed.\n",
      "Gibbs sampling completed.\n",
      "\n",
      "=== Single Document, Single Level Tree ===\n",
      "Level 0: docs=1, total_words=3\n",
      "  Top words: ['apple', 'banana']\n",
      "Document 0:\n",
      "  apple -> level 0\n",
      "  banana -> level 0\n",
      "  apple -> level 0\n",
      "Burn-in period of 2 iterations completed.\n",
      "Iteration 5 completed.\n",
      "Iteration 10 completed.\n",
      "Gibbs sampling completed.\n",
      "\n",
      "=== Small Corpus, Two Levels ===\n",
      "Level 0: docs=2, total_words=0\n",
      "  Top words: []\n",
      "  Level 1: docs=1, total_words=3\n",
      "    Top words: ['cat', 'dog']\n",
      "  Level 1: docs=1, total_words=3\n",
      "    Top words: ['dog', 'cat']\n",
      "Document 0:\n",
      "  cat -> level 1\n",
      "  cat -> level 1\n",
      "  dog -> level 1\n",
      "Document 1:\n",
      "  dog -> level 1\n",
      "  dog -> level 1\n",
      "  cat -> level 1\n"
     ]
    }
   ],
   "source": [
    "# Test Case 1: Small Synthetic Corpus\n",
    "corpus = [\n",
    "    [\"apple\", \"banana\", \"apple\"],\n",
    "    [\"banana\", \"banana\", \"apple\"],\n",
    "    [\"apple\", \"grape\", \"grape\"],\n",
    "    [\"grape\", \"banana\", \"apple\"]\n",
    "]\n",
    "vocab = sorted(set(word for doc in corpus for word in doc))\n",
    "\n",
    "tree = nCRPTree(gamma=1.0, eta=0.1, num_levels=3, vocab=vocab, m=0.5, pi=1.0)\n",
    "tree.gibbs_sampling(corpus, num_iterations=20, burn_in=5, thinning=5)\n",
    "\n",
    "print(\"=== Tree Structure After Gibbs Sampling ===\")\n",
    "print_tree(tree.root, vocab)\n",
    "\n",
    "print(\"\\n=== Document Assignments ===\")\n",
    "for doc_id in range(len(corpus)):\n",
    "    print_document_assignments(tree, doc_id)\n",
    "\n",
    "# Test Case 2: Single Document, Single Level\n",
    "corpus_single = [[\"apple\",\"banana\",\"apple\"]]\n",
    "vocab_single = sorted(set(word for doc in corpus_single for word in doc))\n",
    "tree_single = nCRPTree(gamma=1.0, eta=0.1, num_levels=1, vocab=vocab_single)\n",
    "tree_single.gibbs_sampling(corpus_single, num_iterations=5, burn_in=2, thinning=2)\n",
    "print(\"\\n=== Single Document, Single Level Tree ===\")\n",
    "print_tree(tree_single.root, vocab_single)\n",
    "print_document_assignments(tree_single, 0)\n",
    "\n",
    "# Test Case 3: Check with Minimal Depth and Multiple Docs\n",
    "corpus_small = [\n",
    "    [\"cat\", \"cat\", \"dog\"],\n",
    "    [\"dog\", \"dog\", \"cat\"]\n",
    "]\n",
    "vocab_small = sorted(set(word for doc in corpus_small for word in doc))\n",
    "tree_small = nCRPTree(gamma=1.0, eta=0.1, num_levels=2, vocab=vocab_small)\n",
    "tree_small.gibbs_sampling(corpus_small, num_iterations=10, burn_in=2, thinning=5)\n",
    "print(\"\\n=== Small Corpus, Two Levels ===\")\n",
    "print_tree(tree_small.root, vocab_small)\n",
    "for doc_id in range(len(corpus_small)):\n",
    "    print_document_assignments(tree_small, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tree Structure After Gibbs Sampling ===\n",
      "Tree visualization saved as tree_test_case_1.png\n"
     ]
    }
   ],
   "source": [
    "# Visualisation\n",
    "print(\"=== Tree Structure After Gibbs Sampling ===\")\n",
    "print_tree_graphviz(tree.root, vocab, filename='tree_test_case_1', view=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](../src/tree_test_case_1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA3288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
